<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Test | Jetoile]]></title>
  <link href="https://blog.jetoile.fr/blog/categories/test/atom.xml" rel="self"/>
  <link href="https://blog.jetoile.fr/"/>
  <updated>2017-11-03T19:56:15+01:00</updated>
  <id>https://blog.jetoile.fr/</id>
  <author>
    <name><![CDATA[Khanh Maudoux]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Packaging, test et livraison pour Hadoop : Mode d'emploi]]></title>
    <link href="https://blog.jetoile.fr/2017/11/packaging-et-livraison-pour-hadoop-mode-demploi.html"/>
    <updated>2017-11-03T16:16:05+01:00</updated>
    <id>https://blog.jetoile.fr/2017/11/packaging-et-livraison-pour-hadoop-mode-demploi</id>
    <content type="html"><![CDATA[<p><img src="/images/hadoop-all.png" alt="left-small" /> Hadoop et son écosystème est un monde complexe où beaucoup de nos paradigmes de développeur Java / JavaEE (EE4J?) sont chamboulés.</p>

<p>D'une part les technologies utilisées diffèrent mais, en plus, d'autres questions telles que l'architecture, les tests (unitaires, intégrations, &hellip;), la gestion des logs (debug, audit, pki, &hellip;), les procédures de livraison, la gestion de la configuration de l'application, etc. viennent s'y ajouter.</p>

<p>Cet article va montrer comment il est possible de concilier simplement les tests d'intégration mais aussi le déploiement afin de tendre vers la philosophie de <em>continuous deployment</em>.</p>

<p>Une solution sera proposée et, même si elle est discutable et peut paraitre naïve, elle montrera comment il peut être simple de concilier ces deux points.</p>

<p>Concernant les technologies utilisées, la solution proposée utilisera :</p>

<ul>
<li>Spark 2.2.0</li>
<li>Oozie</li>
<li>Knox</li>
<li>ElasticSearch 5.6.3</li>
<li>Hive</li>
<li>Scala 2.11 pour le langage mais Java pourrait également être utilisé</li>
<li>Maven 3.5.0 pour la partie de <em>build</em></li>
</ul>


<p>Bien sûr, il est facilement possible d'ajouter d'autres technologies comme HBase, Sqoop, Hive (avec exécution de hql) ou autre.</p>

<p>A noter qu'il sera utilisé les composants Hortonworks (HDP 2.6.2) et c'est pourquoi toute la partie exécution des jobs se fera au travers d'Oozie qui est, le plus souvent quand on utilise une distribution du marché, la solution par défaut.</p>

<p>Ainsi il sera traité les points suivants :</p>

<ul>
<li>Description du cas d'usage et implémentation</li>
<li>Anatomie d'un livrable</li>
<li>Mise en oeuvre</li>
</ul>


<!-- more -->


<h1>Description du cas d'usage et implémentation</h1>

<p>L'exemple qui sera utilisée par la suite est simple mais il ne s'agit que d'un prétexte.</p>

<p>Il s'agira de lire des donnnées au format orc, de les indexer dans ElasticSearch et de créer une table Hive dessus. Le tout avec un job Spark.</p>

<p>Ainsi, le code est le suivant :</p>

<p><code>Classe Main</code>
```java
object Main {</p>

<p>  lazy val logger = LoggerFactory.getLogger(this.getClass.getName)</p>

<p>  private def logArguments(reader: String, configPath: String, persister: String) = {
  }</p>

<p>  def parseArgs(args: Array[String]): (String, String, String) = {</p>

<pre><code>val jCommander = new JCommander(CommandLineArgs, args.toArray: _*)

if (CommandLineArgs.help) {
  jCommander.usage()
  System.exit(0)
}
(CommandLineArgs.inputPath, CommandLineArgs.index, CommandLineArgs.docType)
</code></pre>

<p>  }</p>

<p>  def main(args: Array[String]): Unit = {</p>

<pre><code>val sparkSession = SparkSession.builder.appName("simpleApp").config("es.index.auto.create", "true").enableHiveSupport.getOrCreate

val (inputPath, index, docType) = parseArgs(args)

val job = new SimpleJob(sparkSession)
val dataFrame = job.read(inputPath)
dataFrame.cache()
job.write(dataFrame, index, docType)
job.createExternalTable(dataFrame, "tmp", inputPath + "/tmp")
</code></pre>

<p>  }
}
```</p>

<p><code>Classe SimpleJob</code>
```scala
class SimpleJob(sc: SparkSession) {
  val sparkSession = sc</p>

<p>  def read(path: String): DataFrame = {</p>

<pre><code>sparkSession.read.format("orc").load(path)
</code></pre>

<p>  }</p>

<p>  def write(dataFrame: DataFrame, index: String, docType: String): Unit = {</p>

<pre><code>import sparkSession.implicits._
val rdd = dataFrame.map(t =&gt; new FooData(t.getAs("id"), t.getAs("value")))

import org.elasticsearch.spark._
rdd.rdd.saveToEs(index + "/" + docType)
</code></pre>

<p>  }</p>

<p>  def createExternalTable(dataframe: DataFrame, hiveTableName: String, location: String) = {</p>

<pre><code>dataframe.registerTempTable("my_temp_table")
sparkSession.sql("CREATE EXTERNAL TABLE " + hiveTableName + " (id STRING, value STRING) STORED AS ORC LOCATION '" + location + "'")
sparkSession.sql("INSERT INTO " + hiveTableName + " SELECT * from my_temp_table")
</code></pre>

<p>  }</p>

<p>  def shutdown(): Unit = {</p>

<pre><code>if (sparkSession != null) {
  sparkSession.stop()
}
</code></pre>

<p>  }
}
```</p>

<blockquote><p>On notera que la création de la session Spark a été dissociée du job afin de permettre la réalisation de tests d'intégration.</p></blockquote>

<p>Afin de réaliser des tests d'intégration, <a href="https://github.com/jetoile/hadoop-unit">Hadoop Unit</a> est utilisé avec le mode plugin maven en mode embedded.</p>

<p>Ainsi les tests d'intégrations donnent cela:
<code>Classe SimpleJobIntegrationTest</code>
```scala
class SimpleJobIntegrationTest extends FeatureSpec with BeforeAndAfterAll with BeforeAndAfter with GivenWhenThen {</p>

<p>  var configuration: Configuration = _
  val inputCsvPath: String = &ldquo;/input/csv&rdquo;
  val inputOrcPath: String = &ldquo;/input/orc&rdquo;
  val index: String = &ldquo;test_index&rdquo;
  val docType: String = &ldquo;foo&rdquo;
  var DROP_TABLES: Operation = _</p>

<p>  override protected def beforeAll(): Unit = {</p>

<pre><code>HadoopUtils.INSTANCE

configuration = new PropertiesConfiguration(HadoopUnitConfig.DEFAULT_PROPS_FILE)

DROP_TABLES = sequenceOf(sql("DROP TABLE IF EXISTS default.toto"));
</code></pre>

<p>  }</p>

<p>  before {</p>

<pre><code>val fileSystem = HdfsUtils.INSTANCE.getFileSystem

val hdfsPath = "hdfs://" + configuration.getString(HDFS_NAMENODE_HOST_KEY) + ":" + configuration.getInt(HDFS_NAMENODE_PORT_KEY) + "/"

val created = fileSystem.mkdirs(new Path(hdfsPath + inputCsvPath))

fileSystem.copyFromLocalFile(new Path(SimpleJobIntegrationTest.this.getClass.getClassLoader.getResource("simplefile.csv").toURI), new Path(hdfsPath + inputCsvPath + "/simplefile.csv"))

val sparkSession = SparkSession.builder.appName("test").master("local[*]").enableHiveSupport.getOrCreate

val dataFrame = sparkSession.read.format("com.databricks.spark.csv")
  .option("header", "true")
  .option("delimiter", ",")
  .load(hdfsPath + inputCsvPath + "/simplefile.csv")

dataFrame.write.option("orc.compress", "ZLIB")
  .mode(SaveMode.Append)
  .orc(hdfsPath + inputOrcPath + "/simplefile.orc")

sparkSession.stop()
</code></pre>

<p>  }</p>

<p>  after {</p>

<pre><code>new HiveSetup(HiveConnectionUtils.INSTANCE.getDestination, sequenceOf(DROP_TABLES)).launch()
HdfsUtils.INSTANCE.getFileSystem().delete(new Path("/input"))
</code></pre>

<p>  }</p>

<p>  feature(&ldquo;simple test&rdquo;) {</p>

<pre><code>scenario("read data") {

  Given("a local spark conf")
  val sparkSession = SparkSession.builder.appName("test").master("local[*]").enableHiveSupport.getOrCreate

  And("my job")
  val job = new SimpleJob(sparkSession)

  When("I read an orc")
  val hdfsPath = "hdfs://" + configuration.getString(HDFS_NAMENODE_HOST_KEY) + ":" + configuration.getInt(HDFS_NAMENODE_PORT_KEY) + "/"
  val dataFrame = job.read(hdfsPath + inputOrcPath + "/simplefile.orc")

  Then("I have the right schema")
  assertThat(dataFrame.schema.fieldNames).contains("id", "value")

  job.shutdown()
}

scenario("write into es") {

  Given("a local spark conf")
  val sparkSession = SparkSession.builder
    .appName("test")
    .master("local[*]")
    .config("spark.driver.allowMultipleContexts", "true")
    .config("es.index.auto.create", "true")
    .config("es.nodes", configuration.getString(ELASTICSEARCH_IP_KEY))
    .config("es.port", configuration.getString(ELASTICSEARCH_HTTP_PORT_KEY))
    .config("es.nodes.wan.only", "true")
    .enableHiveSupport
    .getOrCreate

  And("my job")
  val job = new SimpleJob(sparkSession)

  When("I read an orc")
  val hdfsPath = "hdfs://" + configuration.getString(HDFS_NAMENODE_HOST_KEY) + ":" + configuration.getInt(HDFS_NAMENODE_PORT_KEY) + "/"
  val dataFrame = job.read(hdfsPath + inputOrcPath + "/simplefile.orc")

  And("I call write method")
  job.write(dataFrame, index, docType)
  job.shutdown()

  Then("data is indexed into ES")
  val client = new PreBuiltTransportClient(Settings.EMPTY).addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(configuration.getString(ELASTICSEARCH_IP_KEY)), configuration.getInt(ELASTICSEARCH_TCP_PORT_KEY)))

  client.admin.indices.prepareRefresh(index).execute.actionGet

  val response = client.prepareSearch(index)
    .setTypes(docType)
    .setSize(0)
    .setQuery(QueryBuilders.queryStringQuery("*")).get().getHits().getTotalHits();

  assertThat(response).isEqualTo(3)
}

scenario("create external table") {

  Given("a local spark conf")
  val sparkSession = SparkSession.builder.appName("test").master("local[*]").enableHiveSupport.getOrCreate

  And("my job")
  val job = new SimpleJob(sparkSession)

  When("I read an orc")
  val hdfsPath = "hdfs://" + configuration.getString(HDFS_NAMENODE_HOST_KEY) + ":" + configuration.getInt(HDFS_NAMENODE_PORT_KEY) + "/"
  val dataFrame = job.read(hdfsPath + inputOrcPath + "/simplefile.orc")

  And("I call createExternalTable method")
  job.createExternalTable(dataFrame, "default.toto", hdfsPath + "input/titi")
  job.shutdown()

  Then("my external table is created")
  val stmt = HiveConnectionUtils.INSTANCE.getConnection.createStatement
  val resultSet = stmt.executeQuery("SELECT * FROM default.toto")
  while (resultSet.next) {
    val id = resultSet.getInt(1)
    val value = resultSet.getString(2)
    assertThat(id).isNotNull
    assertThat(value).isNotNull
  }
}
</code></pre>

<p>  }
}
```</p>

<blockquote><p>On peut noter que la partie <em>before</em> charge un fichier csv dans hdfs et le transforme en orc avec un job Spark (un simple read/write). En effet, il est plus aisé de lire un csv qu'un fichier orc pour un être humain&hellip; ;)</p>

<p>On peut également noter la suppression de la table Hive créée sur la partie <em>after</em> qui utilise DbSetup <em>wrappé</em> par Hadoop Unit.</p>

<p>Enfin, on peut constater que les job Spark utilisés ici sont bien en mode <strong>local</strong>.</p></blockquote>

<p>Afin de permettre le démarrage d'Hadoop Unit en phase de pré-integration test et son arrêt en phase de post-integration test, la configuration maven est la suivante :
<code>pom.xml</code>
```xml</p>

<p>  <profiles></p>

<pre><code>&lt;profile&gt;
  &lt;id&gt;windows&lt;/id&gt;
  &lt;activation&gt;
    &lt;os&gt;
      &lt;family&gt;windows&lt;/family&gt;
    &lt;/os&gt;
  &lt;/activation&gt;
  &lt;properties&gt;
    &lt;suffix.test&gt;"(?&amp;lt;!Integration)(Test|Case|Suite|Spec)"&lt;/suffix.test&gt;
    &lt;suffix.it&gt;"(?&amp;lt;=Integration)(Test|Case|Suite|Spec)"&lt;/suffix.it&gt;
  &lt;/properties&gt;
&lt;/profile&gt;
&lt;profile&gt;
  &lt;id&gt;default&lt;/id&gt;
  &lt;activation&gt;
    &lt;activeByDefault&gt;true&lt;/activeByDefault&gt;
  &lt;/activation&gt;
  &lt;properties&gt;
    &lt;suffix.test&gt;(?&amp;lt;!Integration)(Test|Case|Suite|Spec)&lt;/suffix.test&gt;
    &lt;suffix.it&gt;(?&amp;lt;=Integration)(Test|Case|Suite|Spec)&lt;/suffix.it&gt;
  &lt;/properties&gt;
&lt;/profile&gt;
</code></pre>

<p>  </profiles></p>

<p>  <build></p>

<pre><code>&lt;plugins&gt;
  &lt;plugin&gt;
    &lt;groupId&gt;org.scalatest&lt;/groupId&gt;
    &lt;artifactId&gt;scalatest-maven-plugin&lt;/artifactId&gt;
    &lt;version&gt;${scalatest.maven.plugin.version}&lt;/version&gt;
    &lt;executions&gt;
      &lt;execution&gt;
        &lt;id&gt;IntegrationTest&lt;/id&gt;
        &lt;goals&gt;
          &lt;goal&gt;test&lt;/goal&gt;
        &lt;/goals&gt;
        &lt;phase&gt;integration-test&lt;/phase&gt;
        &lt;configuration&gt;
          &lt;suffixes&gt;${suffix.it}&lt;/suffixes&gt;
        &lt;/configuration&gt;
      &lt;/execution&gt;
      &lt;execution&gt;
        &lt;id&gt;UnitTest&lt;/id&gt;
        &lt;goals&gt;
          &lt;goal&gt;test&lt;/goal&gt;
        &lt;/goals&gt;
        &lt;phase&gt;test&lt;/phase&gt;
        &lt;configuration&gt;
          &lt;suffixes&gt;${suffix.test}&lt;/suffixes&gt;
        &lt;/configuration&gt;
      &lt;/execution&gt;
    &lt;/executions&gt;
  &lt;/plugin&gt;

  &lt;plugin&gt;
    &lt;artifactId&gt;hadoop-unit-maven-plugin&lt;/artifactId&gt;
    &lt;groupId&gt;fr.jetoile.hadoop&lt;/groupId&gt;
    &lt;version&gt;${hadoop-unit.version}&lt;/version&gt;
    &lt;executions&gt;
      &lt;execution&gt;
        &lt;id&gt;start&lt;/id&gt;
        &lt;goals&gt;
          &lt;goal&gt;embedded-start&lt;/goal&gt;
        &lt;/goals&gt;
        &lt;phase&gt;pre-integration-test&lt;/phase&gt;
      &lt;/execution&gt;
      &lt;execution&gt;
        &lt;id&gt;stop&lt;/id&gt;
        &lt;goals&gt;
          &lt;goal&gt;embedded-stop&lt;/goal&gt;
        &lt;/goals&gt;
        &lt;phase&gt;post-integration-test&lt;/phase&gt;
      &lt;/execution&gt;
    &lt;/executions&gt;
    &lt;configuration&gt;
      &lt;components&gt;
        &lt;componentArtifact implementation="fr.jetoile.hadoopunit.ComponentArtifact"&gt;
          &lt;componentName&gt;HDFS&lt;/componentName&gt;
          &lt;artifact&gt;fr.jetoile.hadoop:hadoop-unit-hdfs:${hadoop-unit.version}&lt;/artifact&gt;
        &lt;/componentArtifact&gt;
        &lt;componentArtifact implementation="fr.jetoile.hadoopunit.ComponentArtifact"&gt;
          &lt;componentName&gt;ZOOKEEPER&lt;/componentName&gt;
          &lt;artifact&gt;fr.jetoile.hadoop:hadoop-unit-zookeeper:${hadoop-unit.version}&lt;/artifact&gt;
        &lt;/componentArtifact&gt;
        &lt;componentArtifact implementation="fr.jetoile.hadoopunit.ComponentArtifact"&gt;
          &lt;componentName&gt;HIVEMETA&lt;/componentName&gt;
          &lt;artifact&gt;fr.jetoile.hadoop:hadoop-unit-hive:${hadoop-unit.version}&lt;/artifact&gt;
        &lt;/componentArtifact&gt;
        &lt;componentArtifact implementation="fr.jetoile.hadoopunit.ComponentArtifact"&gt;
          &lt;componentName&gt;HIVESERVER2&lt;/componentName&gt;
          &lt;artifact&gt;fr.jetoile.hadoop:hadoop-unit-hive:${hadoop-unit.version}&lt;/artifact&gt;
        &lt;/componentArtifact&gt;
        &lt;componentArtifact implementation="fr.jetoile.hadoopunit.ComponentArtifact"&gt;
          &lt;componentName&gt;ELASTICSEARCH&lt;/componentName&gt;
          &lt;artifact&gt;fr.jetoile.hadoop:hadoop-unit-elasticsearch:${hadoop-unit.version}&lt;/artifact&gt;
          &lt;properties&gt;
            &lt;elasticsearch.version&gt;${elasticsearch.version}&lt;/elasticsearch.version&gt;
            &lt;elasticsearch.download.url&gt;
              https://download.elastic.co/elasticsearch/release/org/elasticsearch/distribution/zip/elasticsearch/${elasticsearch.version}/elasticsearch-${elasticsearch.version}.zip
            &lt;/elasticsearch.download.url&gt;
          &lt;/properties&gt;
        &lt;/componentArtifact&gt;
      &lt;/components&gt;
    &lt;/configuration&gt;
  &lt;/plugin&gt;

  &lt;plugin&gt;
    &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;
    &lt;version&gt;${maven-shade-plugin.version}&lt;/version&gt;
    &lt;executions&gt;
      &lt;execution&gt;
        &lt;phase&gt;package&lt;/phase&gt;
        &lt;goals&gt;
          &lt;goal&gt;shade&lt;/goal&gt;
        &lt;/goals&gt;
        &lt;configuration&gt;
          &lt;filters&gt;
            &lt;filter&gt;
              &lt;artifact&gt;*:*&lt;/artifact&gt;
              &lt;excludes&gt;
                &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt;
                &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt;
                &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt;
              &lt;/excludes&gt;
            &lt;/filter&gt;
          &lt;/filters&gt;
          &lt;shadedClassifierName&gt;uber&lt;/shadedClassifierName&gt;
          &lt;shadedArtifactAttached&gt;false&lt;/shadedArtifactAttached&gt;
        &lt;/configuration&gt;
      &lt;/execution&gt;
    &lt;/executions&gt;
  &lt;/plugin&gt;

&lt;/plugins&gt;
</code></pre>

<p>  </build>
```</p>

<blockquote><p>On peut noter 3 choses dans ce <code>pom.xml</code> :</p>

<ul>
<li>la configuration de tous ce qui est post-fixé par Integration(Test|Case|Suite|Spec) dans le scope d'integration test. Cela permet de séparer les tests unitaires des tests d'intégration</li>
<li>l'utilisation d'un <em>profile</em> afin de corriger un bug sous windows obligeant à <em>quoter</em> le suffixe lors de l'utilisation de scalatest</li>
<li>l'utilisation du plugin shade afin de générer un <em>fat jar</em> ayant le nom de l'artéfact</li>
</ul>


<p>Ainsi, l'exécution de <code>mvn test</code> n'exécutera que les tests unitaires alors que les tests d'intégration seront exécutés dès la phase integration-test (par exemple avec <code>mvn install</code> ou <code>mvn verify</code>).</p></blockquote>

<h1>Anatomie d'un livrable</h1>

<p>Dans le paragraphe précédent, il a été montré comment il était possible de réaliser des tests d'intégrations sur un job Spark nécessitant la présence de Hdfs, du métastore Hive et d'ElasticSearch.</p>

<p>Un <em>fat jar</em> a également été produit.</p>

<p>Dans ce paragraphe, nous allons voir de quoi nous avons besoin pour permettre de le déployer sur le cluster.</p>

<p>En fait, lorsqu'il est décidé de tout orchestrer via Oozie, il devient nécessaire de pousser les jars (ou les scripts) dans un répertoire Hdfs donné mais également le workflow oozie (et éventuellement son/ses coordinateur(s)). Il est alors possible de <em>submitter</em> le workflow (ou le coordinateur) au travers du serveur Oozie en lui précisant un fichier <code>job.properties</code>.</p>

<p>Ainsi, dans notre cas, le livrable sera constitué de :</p>

<ul>
<li>un jar (job Spark)</li>
<li>un workflow.xml référençant le jar dans un répertoire Hdfs</li>
<li>un coordinateur référençant le workflow dans un répertoire Hdfs</li>
<li>un job.properties permettant d'exécuter le coordinateur</li>
<li>un job.properties permettant d'exécuter le workflow (parce qu'on ne sait jamais si le job doit être exécuté manuellement)</li>
</ul>


<p>De plus, si il existe différent environnement (ie. developpement, recette ou production), il est préférable de variabiliser les choses spécifiques aux environnements (ex: répertoire Hdfs, nom de database Hive, nom d'index ElasticSearch, &hellip;)</p>

<p>Ainsi, il peut être imaginé le format suivant :</p>

<p><code>bash
├── dist
│   ├── lib
│   │   └── bigdata-sample-job-1.0-SNAPSHOT.jar
│   └── oozie
│       ├── coordinator
│       │   └── sample
│       │       └── coordinator.xml
│       ├── run
│       │   ├── coordinator
│       │   │   └── sample
│       │   │       └── job.properties
│       │   └── workflow
│       │       └── sample
│       │           └── job.properties
│       └── workflow
│           └── sample
│               └── workflow.xml
</code></p>

<p>où le <code>workflow.xml</code> pourrait être le suivant :</p>

<p>```xml
&lt;workflow-app name=&ldquo;sample&rdquo; xmlns=&ldquo;uri:oozie:workflow:0.5&rdquo;>
  <global></p>

<pre><code>&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;mapred.job.queue.name&lt;/name&gt;
    &lt;value&gt;queue&lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>

<p>  </global>
  <start to="start"/>
  <kill name="kill"></p>

<pre><code>&lt;message&gt;Action failed, error message[${wf:errorMessage(wf:lastErrorNode())}]&lt;/message&gt;
</code></pre>

<p>  </kill>
  <action name="start"></p>

<pre><code>&lt;spark xmlns="uri:oozie:spark-action:0.2"&gt;
  &lt;job-tracker&gt;${jobTracker}&lt;/job-tracker&gt;
  &lt;name-node&gt;${nameNode}&lt;/name-node&gt;
  &lt;master&gt;yarn&lt;/master&gt;
  &lt;mode&gt;cluster&lt;/mode&gt;
  &lt;name&gt;sampleSpark&lt;/name&gt;
  &lt;class&gt;fr.jetoile.hadoop.sample.Main&lt;/class&gt;
  &lt;jar&gt;${nameNode}//user/myuser/projects/share/lib/bigdata-sample-job-1.0-SNAPSHOT.jar&lt;/jar&gt;
  &lt;arg&gt;-p&lt;/arg&gt;
  &lt;arg&gt;hdfs://namenode/mypath&lt;/arg&gt;
  &lt;arg&gt;-i&lt;/arg&gt;
  &lt;arg&gt;sampleIndex&lt;/arg&gt;
  &lt;arg&gt;-d&lt;/arg&gt;
  &lt;arg&gt;sampleDoc&lt;/arg&gt;
&lt;/spark&gt;
&lt;ok to="End"/&gt;
&lt;error to="kill"/&gt;
</code></pre>

<p>  </action>
  <end name="End"/>
&lt;/workflow-app>
```</p>

<p>et le <code>job.properties</code> servant à l'exécuter :</p>

<p><code>bash
oozie.use.system.libpath=True
send_email=False
dryrun=False
security_enabled=False
nameNode=hdfs://namenode
jobTracker=jobtracker:8050
oozie.wf.application.path=hdfs://namenode/user/myuser/projects/oozie/workflow/sample/
</code></p>

<p>Par contre, savoir ce qui doit être déployé sur Hdfs est intéressant mais il faut ensuite réaliser le déploiement en lui même.</p>

<p>Ainsi, dans notre exemple, il faut :</p>

<ul>
<li>copier le fat jar dans le répertoire Hdfs <code>/user/myuser/projects/share/lib/</code> au travers de Knox (l'accès WebHdfs ou le client Hdfs n'étant souvent pas accessible pour des raisons de sécurité)</li>
<li>copier le workflow.xml dans le répertoire Hdfs <code>/user/myuser/projects/oozie/workflow/sample</code> au travers de Knox (l'accès WebHdfs ou le client Hdfs n'étant souvent pas accessible pour des raisons de sécurité)</li>
<li>exécuter le <code>workflow.xml</code> via la submission du fichier <code>job.properties</code> au Server Oozie au travers de Knox (l'accès au client Oozie ou à l'API Rest de Oozie n'étant souvent pas accessible pour des raisons de sécurité)</li>
</ul>


<p>Bien sûr, il est possible d'exécuter une suite de <code>curl</code> afin de permettre cela. Cependant, cela peut vite s'avérer réberbatif et, surtout, source d'erreur&hellip;</p>

<blockquote><p>Pour rappel, par exemple, la copie d'un fichier sur Hdfs au travers de WebHdfs ou Knox se fait au travers 2 appels http (un premier PUT avec l'option CREATE qui renvoie dans le header une <em>location</em> ainsi qu'un code retour 307 puis un second PUT avec le fichier à l'url retournée dans le header <em>location</em> de l'appel précédent).</p></blockquote>

<p>Ainsi, il peut être intéressant de regarder le composant <strong>gateway shell</strong> fournit par Knox et qui offre la possibilité d'écrire un script Groovy (ou un programme Java/Scala) permettant de piloter ces opérations de manière un peu plus <em>lisible</em> (une copie de fichier sur Hdfs devient <code>Hdfs.put(session).file(&lt;fichier&gt;).to(&lt;path hdfs&gt;).now</code>).</p>

<blockquote><p>Il devient alors possible de fournir un ensemble de méthodes statiques utilisables, par exemple, par un script groovy où serait décrit la procédure de déploiement. Ces méthodes statiques pourraient alors faire un ensemble de vérification (comme, par exemple, vérifier qu'il n'existe pas déjà les fichiers dans Hdfs et faire un renommage dans le cas contraire)</p></blockquote>

<p>Ce script de déploiement pourrait être le suivant :
```groovy
deployOnHdfs(&ldquo;dist/lib&rdquo;, &ldquo;/user/myuser/projects/share/lib/&rdquo;, config, options)
deployOnHdfs(&ldquo;dist/oozie&rdquo;, &ldquo;/user/myuser/projects/oozie/&rdquo;, config, options)</p>

<p>runOozieJobs(&ldquo;dist/oozie/run/workflow/sample/job.properties&rdquo;, config, options)</p>

<p>//deleteOnHdfs(&ldquo;/user/myuser/projects/share/lib&rdquo;, config, options)</p>

<p>//def w = checkOozieCoordinatorStatus(config, options, &ldquo;status=RUNNING&rdquo;)
//killOrExitCoordinators(&ldquo;SAMPLE COORD&rdquo;, w, options, config)
```</p>

<p>Ainsi, il a été défini ce qu'était un livrable qui est donc constitué de :</p>

<ul>
<li>un jar</li>
<li>un ensemble de fichiers oozie</li>
<li>un script de déploiement indiquant quoi copier où et ce qu'il doit exécuter.</li>
</ul>


<blockquote><p>A noter que Knox propose la même API que WebHdfs et que Oozie. Il est donc possible, au besoin, d'attaquer directement WebHdfs et le serveur Oozie (pour rappel, le client Oozie ne fait qu'invoquer des <em>endpoints</em> Rest).</p></blockquote>

<h1>Mise en oeuvre</h1>

<p>Afin de mettre en oeuvre ce qui a été décrit dans les paragraphes précédents mais aussi de rendre réutilisable et un peu plus industrialisable les choses, il est possible de découper les choses en 3 repositories :</p>

<ul>
<li>un repository (<strong>parent</strong>) ne contenant que la configuration maven qui sera hérité par les projets (versions des archetypes configurées en <em>dependencyManagement</em>, configuration des plugins (scala en l'occurence et séparation tests unitaires/tests d'intégrations), &hellip;)</li>
<li>un repository (<strong>commons</strong>) contenant 2 modules maven :

<ul>
<li>un module (<strong>commons-conf</strong>) contenant la configuration globale (namenode, resource manager, knox, hivemetastore, &hellip;) du cluster hadoop par environnement (developpement, recette, production)</li>
<li>un module (<strong>commons-deploy</strong>) contenant les méthodes statiques groovy utilisées dans les scripts de déploiement</li>
</ul>
</li>
<li>un repository (<strong>sample</strong>) qui contenant au moins 2 modules maven :

<ul>
<li>un module (<strong>sample-conf</strong>) contenant les configurations  spécifiques projet (configuration, workflow, coordinator, &hellip;) ainsi que les scripts de déploimenet permettant un déploiement sur le cluster</li>
<li>un ou des module(s) (<strong>sample-job</strong>) contenant le code des jobs spark ou autre</li>
</ul>
</li>
</ul>


<p>De plus, afin de permettre d'exécuter le script de déploiment (écrit dans groovy dans notre cas), il faut proposer un script shell (type sh) qui appelle un programme Java encapsulant le moteur Groovy. En effet, il est rare que Groovy soit installé sur les différents environnements.</p>

<p>La configuration Maven reste simple pour tous les repositories si ce n'est pour la partie <strong>sample-conf</strong> qui a la responsabilité de construire le livrable.</p>

<p>Pour ce faire, ce module doit :</p>

<ul>
<li>dépendre de <strong>commons-deploy</strong> afin de permettre au script de déploiement d'avoir les méthodes statiques encapsulant la <strong>gateway shell</strong> de Knox</li>
<li>récupérer <strong>commons-conf</strong>, dézipper le jar et mettre à disposition son contenu afin de pouvoir remplacer les variables des différents fichiers</li>
<li>remplacer les variables des fichiers par les configurations spécifiques projet et global</li>
<li>générer le script sh appelant la classe Java permettant d'exécuter le script groovy</li>
<li>générer le livrable (au format tar.gz par exemple)</li>
</ul>


<p>Enfin, cerise sur le gateau, il peut également être utile de positionner les <strong>start_date</strong> des coordinateurs Oozie à la date de génération du livrable.</p>

<p>Pour ce faire, une simple combinaison des bons plugins maven permet d'obtenir le résultat escompté :</p>

<p>```xml
  <build></p>

<pre><code>&lt;resources&gt;
  &lt;resource&gt;
    &lt;directory&gt;src/main/resources&lt;/directory&gt;
    &lt;filtering&gt;true&lt;/filtering&gt;
  &lt;/resource&gt;
&lt;/resources&gt;
&lt;plugins&gt;
  &lt;plugin&gt;
    &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt;
    &lt;executions&gt;
      &lt;execution&gt;
        &lt;id&gt;unpack&lt;/id&gt;
        &lt;phase&gt;generate-resources&lt;/phase&gt;
        &lt;goals&gt;
          &lt;goal&gt;unpack&lt;/goal&gt;
        &lt;/goals&gt;
        &lt;configuration&gt;
          &lt;artifactItems&gt;
            &lt;artifactItem&gt;
              &lt;groupId&gt;fr.jetoile.hadoop.sample&lt;/groupId&gt;
              &lt;artifactId&gt;bigdata-commons-conf&lt;/artifactId&gt;
              &lt;type&gt;jar&lt;/type&gt;
              &lt;overWrite&gt;false&lt;/overWrite&gt;
              &lt;outputDirectory&gt;${project.build.directory}/global-conf&lt;/outputDirectory&gt;
            &lt;/artifactItem&gt;
          &lt;/artifactItems&gt;
        &lt;/configuration&gt;
      &lt;/execution&gt;
    &lt;/executions&gt;
  &lt;/plugin&gt;

  &lt;plugin&gt;
    &lt;groupId&gt;org.codehaus.gmaven&lt;/groupId&gt;
    &lt;artifactId&gt;groovy-maven-plugin&lt;/artifactId&gt;
    &lt;dependencies&gt;
      &lt;dependency&gt;
        &lt;groupId&gt;org.codehaus.groovy&lt;/groupId&gt;
        &lt;artifactId&gt;groovy-all&lt;/artifactId&gt;
        &lt;version&gt;${groovy-all.version}&lt;/version&gt;
      &lt;/dependency&gt;
    &lt;/dependencies&gt;
  &lt;/plugin&gt;

  &lt;plugin&gt;
    &lt;artifactId&gt;maven-resources-plugin&lt;/artifactId&gt;
    &lt;executions&gt;
      &lt;execution&gt;
        &lt;id&gt;dev-resources&lt;/id&gt;
        &lt;phase&gt;process-resources&lt;/phase&gt;
        &lt;goals&gt;
          &lt;goal&gt;resources&lt;/goal&gt;
        &lt;/goals&gt;
        &lt;configuration&gt;
          &lt;outputDirectory&gt;${project.build.outputDirectory}/dev&lt;/outputDirectory&gt;
          &lt;filters&gt;
            &lt;filter&gt;${basedir}/src/main/filters/dev/conf.properties&lt;/filter&gt;
            &lt;filter&gt;${project.build.directory}/global-conf/dev/conf.properties&lt;/filter&gt;
          &lt;/filters&gt;
        &lt;/configuration&gt;
      &lt;/execution&gt;
      &lt;execution&gt;
        &lt;id&gt;prod-resources&lt;/id&gt;
        &lt;phase&gt;process-resources&lt;/phase&gt;
        &lt;goals&gt;
          &lt;goal&gt;resources&lt;/goal&gt;
        &lt;/goals&gt;
        &lt;configuration&gt;
          &lt;outputDirectory&gt;${project.build.outputDirectory}/prod&lt;/outputDirectory&gt;
          &lt;filters&gt;
            &lt;filter&gt;${basedir}/src/main/filters/prd/conf.properties&lt;/filter&gt;
            &lt;filter&gt;${project.build.directory}/global-conf/prd/conf.properties&lt;/filter&gt;
          &lt;/filters&gt;
        &lt;/configuration&gt;
      &lt;/execution&gt;
      &lt;execution&gt;
        &lt;id&gt;test-resources&lt;/id&gt;
        &lt;phase&gt;process-resources&lt;/phase&gt;
        &lt;goals&gt;
          &lt;goal&gt;resources&lt;/goal&gt;
        &lt;/goals&gt;
        &lt;configuration&gt;
          &lt;outputDirectory&gt;${project.build.outputDirectory}/test&lt;/outputDirectory&gt;
          &lt;filters&gt;
            &lt;filter&gt;${basedir}/src/main/filters/test/conf.properties&lt;/filter&gt;
            &lt;filter&gt;${project.build.directory}/global-conf/test/conf.properties&lt;/filter&gt;
          &lt;/filters&gt;
        &lt;/configuration&gt;
      &lt;/execution&gt;
    &lt;/executions&gt;
  &lt;/plugin&gt;

  &lt;plugin&gt;
    &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt;
    &lt;artifactId&gt;appassembler-maven-plugin&lt;/artifactId&gt;
    &lt;version&gt;1.10&lt;/version&gt;
    &lt;executions&gt;
      &lt;execution&gt;
        &lt;goals&gt;
          &lt;goal&gt;assemble&lt;/goal&gt;
        &lt;/goals&gt;
        &lt;phase&gt;package&lt;/phase&gt;
      &lt;/execution&gt;
    &lt;/executions&gt;
    &lt;configuration&gt;
      &lt;configurationDirectory&gt;conf&lt;/configurationDirectory&gt;
      &lt;programs&gt;
        &lt;program&gt;
          &lt;mainClass&gt;Main&lt;/mainClass&gt;
          &lt;name&gt;main&lt;/name&gt;
        &lt;/program&gt;
      &lt;/programs&gt;
      &lt;repositoryLayout&gt;flat&lt;/repositoryLayout&gt;
      &lt;binFileExtensions&gt;
        &lt;unix&gt;.sh&lt;/unix&gt;
      &lt;/binFileExtensions&gt;
    &lt;/configuration&gt;
  &lt;/plugin&gt;

  &lt;plugin&gt;
    &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;
    &lt;executions&gt;
      &lt;execution&gt;
        &lt;id&gt;deploy-dev&lt;/id&gt;
        &lt;phase&gt;package&lt;/phase&gt;
        &lt;goals&gt;
          &lt;goal&gt;single&lt;/goal&gt;
        &lt;/goals&gt;
        &lt;configuration&gt;
          &lt;descriptors&gt;
            &lt;descriptor&gt;src/main/assembly/descriptor-deploy-run-dev.xml&lt;/descriptor&gt;
          &lt;/descriptors&gt;
        &lt;/configuration&gt;
      &lt;/execution&gt;
      &lt;execution&gt;
        &lt;id&gt;deploy-prd&lt;/id&gt;
        &lt;phase&gt;package&lt;/phase&gt;
        &lt;goals&gt;
          &lt;goal&gt;single&lt;/goal&gt;
        &lt;/goals&gt;
        &lt;configuration&gt;
          &lt;descriptors&gt;
            &lt;descriptor&gt;src/main/assembly/descriptor-deploy-run-prd.xml&lt;/descriptor&gt;
          &lt;/descriptors&gt;
        &lt;/configuration&gt;
      &lt;/execution&gt;
      &lt;execution&gt;
        &lt;id&gt;deploy-test&lt;/id&gt;
        &lt;phase&gt;package&lt;/phase&gt;
        &lt;goals&gt;
          &lt;goal&gt;single&lt;/goal&gt;
        &lt;/goals&gt;
        &lt;configuration&gt;
          &lt;descriptors&gt;
            &lt;descriptor&gt;src/main/assembly/descriptor-deploy-run-test.xml&lt;/descriptor&gt;
          &lt;/descriptors&gt;
        &lt;/configuration&gt;
      &lt;/execution&gt;

    &lt;/executions&gt;
  &lt;/plugin&gt;


  &lt;plugin&gt;
    &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt;
    &lt;artifactId&gt;buildnumber-maven-plugin&lt;/artifactId&gt;
    &lt;version&gt;1.4&lt;/version&gt;
    &lt;executions&gt;
      &lt;execution&gt;
        &lt;phase&gt;validate&lt;/phase&gt;
        &lt;goals&gt;
          &lt;goal&gt;create-timestamp&lt;/goal&gt;
        &lt;/goals&gt;
      &lt;/execution&gt;
    &lt;/executions&gt;
    &lt;configuration&gt;
      &lt;timestampFormat&gt;yyyy-MM-dd'T'HH:mmZ&lt;/timestampFormat&gt;
      &lt;timestampPropertyName&gt;build.date&lt;/timestampPropertyName&gt;
    &lt;/configuration&gt;
  &lt;/plugin&gt;

&lt;/plugins&gt;
</code></pre>

<p>  </build>
```</p>

<blockquote><p>A noter la configuration du plugin <em>buildnumber-maven-plugin</em> afin d'avoir la valeur de <strong>start_date</strong> au format attendu par Oozie.</p>

<p>A noter l'utilisation du plugin <em>appassembler-maven-plugin</em> permettant la génération du script shell et qui permet de gérer, entre autre, automatiquement la récupération des dépendances nécessaires à l'exécution de la classe Java.</p></blockquote>

<p>Afin de permettre la génération tar.gz du livrable, un assembly maven est utilisable :</p>

<p>```xml
&lt;?xml version=&ldquo;1.0&rdquo;?>
&lt;assembly xmlns=&ldquo;<a href="http://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.2">http://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.2</a>&rdquo;</p>

<pre><code>      xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
      xsi:schemaLocation="http://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.2 http://maven.apache.org/xsd/assembly-1.1.2.xsd"&gt;
</code></pre>

<p>  <id>deploy-dev</id></p>

<p>  <formats></p>

<pre><code>&lt;format&gt;tar.gz&lt;/format&gt;
</code></pre>

<p>  </formats></p>

<p>  <fileSets></p>

<pre><code>&lt;fileSet&gt;
  &lt;!-- all files to deploy have to be into dist directory --&gt;
  &lt;directory&gt;${project.build.outputDirectory}/dev/&lt;/directory&gt;
  &lt;outputDirectory&gt;/dist/&lt;/outputDirectory&gt;
  &lt;excludes&gt;
    &lt;exclude&gt;deploy.properties&lt;/exclude&gt;
  &lt;/excludes&gt;
  &lt;lineEnding&gt;unix&lt;/lineEnding&gt;
&lt;/fileSet&gt;

&lt;fileSet&gt;
  &lt;directory&gt;${project.build.directory}/appassembler&lt;/directory&gt;
  &lt;outputDirectory&gt;/&lt;/outputDirectory&gt;
  &lt;fileMode&gt;750&lt;/fileMode&gt;
  &lt;directoryMode&gt;750&lt;/directoryMode&gt;
&lt;/fileSet&gt;
</code></pre>

<p>  </fileSets></p>

<p>  <dependencySets></p>

<pre><code>&lt;dependencySet&gt;
  &lt;!-- all files to deploy have to be into dist directory --&gt;
  &lt;outputDirectory&gt;/dist/lib/&lt;/outputDirectory&gt;
  &lt;includes&gt;
    &lt;include&gt;
      fr.jetoile.hadoop.sample:bigdata-sample-job
    &lt;/include&gt;
  &lt;/includes&gt;
  &lt;useProjectArtifact&gt;false&lt;/useProjectArtifact&gt;
  &lt;scope&gt;provided&lt;/scope&gt;
  &lt;unpack&gt;false&lt;/unpack&gt;
&lt;/dependencySet&gt;
</code></pre>

<p>  </dependencySets></p>

<p>  <files></p>

<pre><code>&lt;file&gt;
  &lt;source&gt;src/main/groovy/deploy.groovy&lt;/source&gt;
  &lt;outputDirectory&gt;bin&lt;/outputDirectory&gt;
&lt;/file&gt;

&lt;file&gt;
  &lt;source&gt;${project.build.directory}/classes/dev/deploy.properties&lt;/source&gt;
  &lt;outputDirectory&gt;/&lt;/outputDirectory&gt;
&lt;/file&gt;
</code></pre>

<p>  </files></p>

<p></assembly>
```</p>

<blockquote><p>A noter que, afin de récupérer le jar contenant le job Spark, la dépendance a été mis en scopte <em>provided</em>. En effet, le plugin <em>appassembler-maven-plugin</em> prend tout ce qu'il trouve en scope compile et aurait embarqué, par défaut, le jar du job entrainant des conflits de classpath.</p></blockquote>

<h1>Conclusion</h1>

<p>On a vu dans cet article comme il était simple de tester son code et de définir et construire un livrable juste en structurant son code et en utilisant quelques bons plugin maven.</p>

<p>Bien sûr, si cela est faisable en Maven, il est tout à fait possible d'utiliser un autre outils de build pour le faire.</p>

<p>Alors, oui, l'approche est peut être un peu simpliste et naïve mais elle a au moins le mérite d'exister et de proposer à moindre coût quelque chose de facilement généralisable au sein des différents projets/équipes réalisant des développements dans l'écosystème Hadoop.</p>

<p>En outre, le script de déploiement peut également être utilisé pour tester rapidement son code sur le cluster de développement.</p>

<blockquote><p>A noter que le code se trouve dans le <a href="https://github.com/jetoile/bigdata-sample-parent">github ci-joint</a> et qu'il a été mis, pour des questions de simplicité, dans un seul repository (cela explique qu'on ait un pom parent qui ne fait que déclarer les modules fils mais que ces derniers ne le référencent pas).</p>

<p>A noter également qu'un archetype permettant de générer la structure du projet sample est présent (<strong>bigdata-archetype</strong>).</p></blockquote>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Des tests d'intégration avec Cassandra]]></title>
    <link href="https://blog.jetoile.fr/2017/07/tester-avec-cassandra.html"/>
    <updated>2017-07-21T10:22:37+02:00</updated>
    <id>https://blog.jetoile.fr/2017/07/tester-avec-cassandra</id>
    <content type="html"><![CDATA[<p><img src="/images/1200px-Cassandra_logo.svg.png" alt="left-small" /> Parce que je suis parti sur ma lancée des articles <em>des tests d'intégration avec &hellip;</em>, à la demande de <a href="https://twitter.com/doanduyhai">Duyhai</a>, voilà que je me retrouve à faire un article pour Apache Cassandra&hellip; ;)</p>

<p>Plus sérieusement, faire des tests d'intégration avec Apache Cassandra est beaucoup plus simple qu'avec Redis ou Elasticsearch mais il existe cependant 2 projets qui simplifient énormément les tests d'intégration avec Cassandra :</p>

<ul>
<li><a href="https://github.com/jsevellec/cassandra-unit">cassandra-unit</a> créé par <a href="https://github.com/jsevellec">Jérémy Sevellec</a></li>
<li><a href="https://github.com/doanduyhai/Achilles">achille</a> créé par <a href="https://twitter.com/doanduyhai">Duyhai Doan</a></li>
</ul>


<p>Ce petit article résume comment utiliser ces 2 solutions.</p>

<!-- more -->


<h1>Classe de test</h1>

<p>La classe utilisée comme exemple dans les 2 cas est la suivante :</p>

<p>```java
import com.datastax.driver.core.Cluster;
import com.datastax.driver.core.ResultSet;
import com.datastax.driver.core.Row;
import com.datastax.driver.core.Session;
import lombok.AllArgsConstructor;
import lombok.Data;</p>

<p>import java.util.List;
import java.util.stream.Collectors;</p>

<p>public class UserDao {</p>

<pre><code>private Cluster cluster;
private Session session;

public UserDao(String localhost, int port) {
    cluster = Cluster.builder()
            .addContactPoints(localhost)
            .withPort(port)
            .build();

    session = cluster.connect();
}

public List&lt;User&gt; getUsers() {
    ResultSet execute = session.execute("select * from test.user");

    List&lt;Row&gt; res = execute.all();

    return res.stream().map(r -&gt; new User(r.getString("lastName"), r.getString("firstName"))).collect(Collectors.toList());
}
</code></pre>

<p>}</p>

<p>@Data
@AllArgsConstructor
class User {</p>

<pre><code>private String lastName;
private String firstName;
</code></pre>

<p>}
```</p>

<p>Au niveau dépendance, elles sont les suivantes :
```plain
dependencies {</p>

<pre><code>compile group: 'com.datastax.cassandra', name: 'cassandra-driver-core', version:'3.3.0'
compile(group: 'org.projectlombok', name: 'lombok', version:'1.16.18') 
</code></pre>

<p>}
```</p>

<p>Rien de bien compliqué mais il s'agit d'un cas d'exemple ultra simple&hellip; ;)</p>

<h1>Cassandra-Unit</h1>

<p>Pour utiliser Cassandra-Unit, il suffit de déclarer sa dépendance en scope de test:
```xml
<dependency></p>

<pre><code>&lt;groupId&gt;org.cassandraunit&lt;/groupId&gt;
&lt;artifactId&gt;cassandra-unit&lt;/artifactId&gt;
&lt;version&gt;3.1.3.2&lt;/version&gt;
&lt;scope&gt;test&lt;/scope&gt;
</code></pre>

<p></dependency>
```</p>

<p>La classe de test ressemble alors à :
```java
import com.datastax.driver.core.Cluster;
import com.datastax.driver.core.Session;
import org.apache.thrift.transport.TTransportException;
import org.cassandraunit.utils.EmbeddedCassandraServerHelper;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;</p>

<p>import java.io.IOException;
import java.util.List;</p>

<p>import static org.junit.Assert.assertEquals;</p>

<p>public class UserDaoIntegrationTest {</p>

<pre><code>private Session session;
private Cluster cluster;

@Before
public void setUp() throws InterruptedException, IOException, TTransportException {
    EmbeddedCassandraServerHelper.startEmbeddedCassandra("cassandra-sample.yaml");

    cluster = Cluster.builder()
            .addContactPoints("localhost")
            .withPort(9000)
            .build();

    session = cluster.connect();

    session.execute("create KEYSPACE test WITH replication = {'class': 'SimpleStrategy' , 'replication_factor': '1' }");
    session.execute("CREATE TABLE test.user (lastName text, firstName text, PRIMARY KEY (lastName))");
}

@After
public void teardown() {
    session.close();
    EmbeddedCassandraServerHelper.stopEmbeddedCassandra();
}


@Test
public void user_should_be_returned() {
    session.execute("insert into test.user(lastName, firstName) values('lastName1', 'firstName1')");

    UserDao cassandraJob = new UserDao("localhost", 9000);

    List&lt;User&gt; users = cassandraJob.getUsers();

    assertEquals(users.size(), 1);
    assertEquals(users.get(0).getFirstName(), "firstName1");
    assertEquals(users.get(0).getLastName(), "lastName1");
}
</code></pre>

<p>}
```</p>

<p>A noter que le fichier <code>cassandra-sample.yaml</code> correspond au fichier de configuration par défaut de cassandra avec les spécificités suivantes :
```yaml
native_transport_port: 9000
data_file_directories:</p>

<pre><code>- target/embeddedCassandra/data
</code></pre>

<p>commitlog_directory: target/embeddedCassandra/commitlog
hints_directory: target/embeddedCassandra/hints
cdc_raw_directory: target/embeddedCassandra/cdc_raw
saved_caches_directory: target/embeddedCassandra/saved_caches</p>

<h1>multithreaded_compaction: false</h1>

<h1>memtable_flush_queue_size: 4</h1>

<h1>compaction_preheat_key_cache: true</h1>

<h1>in_memory_compaction_limit_in_mb: 64</h1>

<p>```</p>

<p>A noter également que Cassandra-Unit propose également des loader permettant d'initialiser les données dans le cluster ainsi que d'autres facilités d'utilisation telles que des Rules ou un support de Spring. Pour avoir plus d'informations, je vous invite à aller voir la documentation.</p>

<h1>Achille-Embedded</h1>

<p>Pour utiliser Achille-embedded, il suffit de déclarer sa dépendance en scope de test:
```xml
<dependency></p>

<pre><code>&lt;groupId&gt;info.archinnov&lt;/groupId&gt;
&lt;artifactId&gt;achilles-embedded&lt;/artifactId&gt;
&lt;version&gt;5.2.1&lt;/version&gt;
&lt;scope&gt;test&lt;/scope&gt;
</code></pre>

<p></dependency>
```</p>

<p>La classe de test ressemble alors à :
```java
import com.datastax.driver.core.Session;
import info.archinnov.achilles.embedded.CassandraEmbeddedServerBuilder;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;</p>

<p>import java.util.List;</p>

<p>import static org.junit.Assert.assertEquals;</p>

<p>public class UserDaoIntegrationTest {</p>

<pre><code>private Session session;

@Before
public void setUp() {
    session = CassandraEmbeddedServerBuilder.builder()
            .withCQLPort(9000)
            .withClusterName("Test Cluster")
            .cleanDataFilesAtStartup(true)
            .buildNativeSession();

    session.execute("create KEYSPACE test WITH replication = {'class': 'SimpleStrategy' , 'replication_factor': '1' }");
    session.execute("CREATE TABLE test.user (lastName text, firstName text, PRIMARY KEY (lastName))");
}

@After
public void teardown() {
    session.close();
}


@Test
public void user_should_be_returned() {
    session.execute("insert into test.user(lastName, firstName) values('lastName1', 'firstName1')");

    UserDao cassandraJob = new UserDao("localhost", 9000);

    List&lt;User&gt; users = cassandraJob.getUsers();

    assertEquals(users.size(), 1);
    assertEquals(users.get(0).getFirstName(), "firstName1");
    assertEquals(users.get(0).getLastName(), "lastName1");
}
</code></pre>

<p>}
```</p>

<p>A noter également qu'Achille-Embedded propose également des Rules et offre des facilités pour loader des données pour les tests. Pour avoir plus d'informations, je vous invite à aller voir les différents tests du projet.</p>

<h1>Conclusion</h1>

<p>On a donc vu dans ce rapide article comment il était possible de faire des tests d'intégration avec Cassandra.</p>

<p>A noter que le code montré dans cet article est extrêmement simple mais je voulais juste montrer un rapide cas d'usage, du coup, pas grand chose à ajouter puisque les exemples parlent d'eux-même ;) .</p>

<p>Concernant le choix de l'un ou de l'autre, j'avoue avoir une préférence pour Achille-Embedded car il permet plus simplement d'avoir la main sur la configuration et ne pas avoir à maintenir un fichier de configuration en plus.</p>

<p>A noter qu'Achille-Embedded a été utilisé dans <a href="https://github.com/jetoile/hadoop-unit">Hadoop-Unit</a> et qu'il est possible d'utiliser Cassandra en test d'intégration des manière suivantes ( #autopromo ;) ) :</p>

<ul>
<li>En utilisant le démarrage de <a href="https://github.com/jetoile/hadoop-unit/blob/master/hadoop-unit-cassandra/src/test/java/fr/jetoile/hadoopunit/component/CassandraBootstrapTest.java">manière programmatique</a></li>
<li>En utilisant le plugin maven en <a href="https://github.com/jetoile/hadoop-unit/blob/master/sample/spark-streaming-cassandra">phase de pré-integration</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Des tests d'intégration avec Redis]]></title>
    <link href="https://blog.jetoile.fr/2017/07/tester-avec-redis.html"/>
    <updated>2017-07-17T11:10:30+02:00</updated>
    <id>https://blog.jetoile.fr/2017/07/tester-avec-redis</id>
    <content type="html"><![CDATA[<p><img src="/images/redis.png" alt="left-small" /> Redis est écrit en C et faire des tests d'intégration en Java peut s'avérer compliquer. En outre, le fait que Redis doive être compilé lors de son installation rend les choses encore moins aisées.</p>

<p>Bien sûr, il est possible d'utiliser Docker ou de l'installer préalablement sur son poste mais cette deuxième option casse un peu les bonnes pratiques des tests.</p>

<p>Il existe également de nombreux projets permettant de faire des tests avec Redis mais, souvent, les solutions proposées embarquent le binaire de Redis ou on besoin qu'il soit déjà présent et installer/compiler sur le poste (<a href="https://github.com/kstyrc/embedded-redis,">https://github.com/kstyrc/embedded-redis,</a> <a href="https://github.com/lordofthejars/nosql-unit,">https://github.com/lordofthejars/nosql-unit,</a> <a href="https://github.com/ishiis/redis-unit">https://github.com/ishiis/redis-unit</a>). Les solutions qui intègrent le binaire ne sont malheureusement souvent pas à jour et laisse assez peu la main sur la version.</p>

<p>Pour ceux qui n'aurait pas envie d'utiliser Docker, cet article va montrer comment il est possible de piloter programmatiquement l'installation de Redis afin de permettre les tests d'intégration.</p>

<!-- more -->


<p>En fait, le code ci-dessous se charge de télécharger Redis et de le compiler dans le répertoire <code>$USER_HOME/.redis</code>. Pour ce faire une tache ant est utilisée car un appel direct à la commande make échoue.</p>

<p>```java
import org.apache.commons.io.FileUtils;
import org.apache.commons.io.IOUtils;
import org.apache.tools.ant.BuildException;
import org.apache.tools.ant.DefaultLogger;
import org.apache.tools.ant.Project;
import org.apache.tools.ant.ProjectHelper;
import org.codehaus.plexus.archiver.ArchiverException;
import org.codehaus.plexus.archiver.tar.TarGZipUnArchiver;
import org.codehaus.plexus.logging.console.ConsoleLogger;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;</p>

<p>import java.io.File;
import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;
import java.net.URL;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.util.Arrays;
import java.util.List;</p>

<p>import static org.apache.commons.io.FileUtils.getFile;</p>

<p>public class RedisInstaller {</p>

<pre><code>private static final Logger LOGGER = LoggerFactory.getLogger(RedisInstaller.class);
private static final String REDIS_PACKAGE_PREFIX = "redis-";
private static final List&lt;String&gt; REDIS_EXECUTABLE_FILE = Arrays.asList("redis-server", "redis-sentinel");

private static final Path REDIS_INSTALLATION_PATH = Paths.get(System.getProperty("user.home") + "/.redis");

private final String downloadUrl;
private final String version;
private final boolean forceCleanupInstallationDirectory;
private final String tmpDir;

RedisInstaller(String version, String downloadUrl, boolean forceCleanupInstallationDirectory, String tmpDir) {
    this.downloadUrl = downloadUrl;
    this.version = version;
    this.forceCleanupInstallationDirectory = forceCleanupInstallationDirectory;
    this.tmpDir = tmpDir;
    REDIS_INSTALLATION_PATH.toFile().mkdirs();
}

File getExecutableFile() {
    return fileRelativeToInstallationDir("src", "redis-server");
}

private File fileRelativeToInstallationDir(String... path) {
    return getFile(getInstallationDirectory(), path);
}

private File getInstallationDirectory() {
    return getFile(REDIS_INSTALLATION_PATH.toFile(), REDIS_PACKAGE_PREFIX + version);
}

void install() throws IOException, InterruptedException {
    if (forceCleanupInstallationDirectory) {
        FileUtils.forceDelete(getInstallationDirectory());
    }
    installRedis();
    makeRedis();
    applyRedisPermissionRights();
}


private void installRedis() throws IOException {
    Path downloadedTo = download(new URL(downloadUrl + REDIS_PACKAGE_PREFIX + version + ".tar.gz"));
    install(downloadedTo);
}

private File getAntFile() throws IOException {
    InputStream in = RedisInstaller.class.getClassLoader().getResourceAsStream("build.xml");
    File fileOut = new File(tmpDir, "build.xml");

    LOGGER.info("Writing redis' build.xml to: " + fileOut.getAbsolutePath());

    OutputStream out = FileUtils.openOutputStream(fileOut);
    IOUtils.copy(in, out);
    in.close();
    out.close();

    return fileOut;
}

private void makeRedis() throws IOException, InterruptedException {
    LOGGER.info("&gt; make");
    File makeFilePath = getInstallationDirectory();

    DefaultLogger consoleLogger = getConsoleLogger();

    Project project = new Project();
    File buildFile = getAntFile();
    project.setUserProperty("ant.file", buildFile.getAbsolutePath());
    project.addBuildListener(consoleLogger);

    try {
        project.fireBuildStarted();
        project.init();
        ProjectHelper projectHelper = ProjectHelper.getProjectHelper();
        project.addReference("ant.projectHelper", projectHelper);
        project.setProperty("redisDirectory", makeFilePath.getAbsolutePath());
        projectHelper.parse(project, buildFile);
        project.executeTarget("init");
        project.fireBuildFinished(null);
    } catch (BuildException buildException) {
        project.fireBuildFinished(buildException);
        throw new RuntimeException("!!! Unable to compile redis !!!", buildException);
    }
}

private DefaultLogger getConsoleLogger() {
    DefaultLogger consoleLogger = new DefaultLogger();
    consoleLogger.setErrorPrintStream(System.err);
    consoleLogger.setOutputPrintStream(System.out);
    consoleLogger.setMessageOutputLevel(Project.MSG_INFO);

    return consoleLogger;
}

private Path download(URL source) throws IOException {
    File target = new File(REDIS_INSTALLATION_PATH.toString(), source.getPath());
    if (!target.exists()) {
        LOGGER.info("Downloading : " + source + " to " + target + "...");
        FileUtils.copyURLToFile(source, target);
        LOGGER.info("Download complete");
    } else {
        LOGGER.info("Download skipped");
    }
    return target.toPath();
}

private void install(Path downloadedFile) throws IOException {
    LOGGER.info("Installing Redis into " + REDIS_INSTALLATION_PATH + "...");
    try {
        final TarGZipUnArchiver ua = new TarGZipUnArchiver();
        ua.setSourceFile(downloadedFile.toFile());
        ua.enableLogging(new ConsoleLogger(1, "console"));
        ua.setDestDirectory(REDIS_INSTALLATION_PATH.toFile());
        ua.extract();
        LOGGER.info("Done");
    } catch (ArchiverException e) {
        LOGGER.info("Failure : " + e);
        throw new RuntimeException("!!! Unable to download and untar redis !!!", e);
    }
}

private void applyRedisPermissionRights() throws IOException {
    File binDirectory = getFile(getInstallationDirectory(), "src");
    for (String fn : REDIS_EXECUTABLE_FILE) {
        File executableFile = new File(binDirectory, fn);
        LOGGER.info("Applying executable permissions on " + executableFile);
        executableFile.setExecutable(true);
    }
}
</code></pre>

<p>}
```</p>

<p>Avec le fichier ant <code>build.xml</code> suivant :
```xml
&lt;?xml version=&ldquo;1.0&rdquo; encoding=&ldquo;ISO-8859-1&rdquo;?>
<project name="make" default="init" basedir="."></p>

<pre><code>&lt;target name="init"&gt;
    &lt;echo message="Redis compilation is starting" /&gt;
    &lt;exec command="make" dir="${redisDirectory}" /&gt;
    &lt;echo message="Redis compilation finished"/&gt;
&lt;/target&gt;
</code></pre>

<p></project>
```</p>

<p>Ainsi, en wrappant l'appel dans un builder, il devient facile d'invoquer l'installeur.
```java
import lombok.Builder;
import lombok.Data;</p>

<p>import java.io.File;
import java.io.IOException;</p>

<p>@Data
@Builder
public class EmbeddedRedisInstaller {</p>

<pre><code>private String downloadUrl;
private String version;
private String tmpDir;
private boolean forceCleanupInstallationDirectory;

RedisInstaller redisInstaller;

public EmbeddedRedisInstaller install() throws IOException, InterruptedException {
    installRedis();
    return this;
}

private void installRedis() throws IOException, InterruptedException {
    redisInstaller = new RedisInstaller(version, downloadUrl, forceCleanupInstallationDirectory, tmpDir);
    redisInstaller.install();
}

public File getExecutableFile() {
    return redisInstaller.getExecutableFile();
}
</code></pre>

<p>}
```</p>

<p>Utilisation de l'installeur :
```java
EmbeddedRedisInstaller.builder()</p>

<pre><code>            .version("4.0.0")
            .downloadUrl("http://download.redis.io/releases/")
            .forceCleanupInstallationDirectory(false)
            .tmpDir("tmp/redis")
            .build()
    .install();
</code></pre>

<p>```</p>

<p>Enfin, en le couplant, avec le projet <a href="https://github.com/ishiis/redis-unit">redis-unit</a>, il est possible de choisir son mode d'utilisation tout en rendant configurable les ports :
```
EmbeddedRedisInstaller installer = EmbeddedRedisInstaller.builder()</p>

<pre><code>            .downloadUrl(downloadUrl)
            .forceCleanupInstallationDirectory(cleanupInstallation)
            .version(version)
            .tmpDir(tmpDir)
            .build();
</code></pre>

<p>try {</p>

<pre><code>installer.install();
</code></pre>

<p>} catch (IOException | InterruptedException e) {</p>

<pre><code>LOGGER.error("unable to install redis", e);
</code></pre>

<p>}</p>

<p>switch (type) {</p>

<pre><code>case SERVER:
    redisServer = new RedisServer(
            new RedisServerConfig.ServerBuilder(masterPort)
                    .redisBinaryPath(installer.getExecutableFile().getAbsolutePath())
                    .build()
    );
    break;
case CLUSTER:
    redisCluster = new RedisCluster(
            slavePorts.stream().map(p -&gt;
                    new RedisClusterConfig.ClusterBuilder(p)
                            .redisBinaryPath(installer.getExecutableFile().getAbsolutePath())
                            .build()
            ).collect(Collectors.toList())
    );
    break;
case MASTER_SLAVE:
    redisMasterSlave = new RedisMasterSlave(
            new RedisMasterSlaveConfig.MasterBuilder(masterPort)
                    .redisBinaryPath(installer.getExecutableFile().getAbsolutePath())
                    .build(),
            slavePorts.stream().map(p -&gt;
                    new RedisMasterSlaveConfig.SlaveBuilder(p, masterPort)
                            .redisBinaryPath(installer.getExecutableFile().getAbsolutePath())
                            .build()
            ).collect(Collectors.toList())
    );
    break;
case SENTINEL:
    redisSentinel = new RedisSentinel(
            new RedisMasterSlaveConfig.MasterBuilder(masterPort)
                    .redisBinaryPath(installer.getExecutableFile().getAbsolutePath())
                    .build(),
            slavePorts.stream().map(p -&gt;
                    new RedisMasterSlaveConfig.SlaveBuilder(p, masterPort)
                            .redisBinaryPath(installer.getExecutableFile().getAbsolutePath())
                            .build()
            ).collect(Collectors.toList()),
            sentinelPorts.stream().map(p -&gt;
                    new RedisSentinelConfig.SentinelBuilder(p, masterPort)
                            .redisBinaryPath(installer.getExecutableFile().getAbsolutePath())
                            .build()
            ).collect(Collectors.toList())
    );
    break;
</code></pre>

<p>}
```</p>

<h1>Conclusion</h1>

<p>On a pu voir dans cet article comment il était possible simplement d'utiliser Redis lors de ses tests d'intégration sans utiliser Docker et tout en pilotant la chose programmatiquement.</p>

<p>Pour retrouver le code, il est disponible à l'<a href="https://github.com/jetoile/hadoop-unit/tree/master/hadoop-unit-redis">url suivante</a>.</p>

<p>A noter qu'il a été utilisé dans <a href="https://github.com/jetoile/hadoop-unit">Hadoop-Unit</a> et qu'il est possible d'utiliser Redis en test d'intégration des manière suivantes ( #autopromo ;) ) :</p>

<ul>
<li>En utilisant le démarrage de <a href="https://github.com/jetoile/hadoop-unit/blob/master/hadoop-unit-redis/src/test/java/fr/jetoile/hadoopunit/RedisBootstrapTest.java">manière programmatique</a>:
```
@BeforeClass
public static void setup() throws BootstrapException {
  HadoopBootstrap.INSTANCE.startAll();
}</li>
</ul>


<p>@AfterClass
public static void tearDown() throws BootstrapException {</p>

<pre><code>HadoopBootstrap.INSTANCE.stopAll();
</code></pre>

<p>}</p>

<p>@Test
public void testStartAndStopServerMode() throws InterruptedException {</p>

<pre><code>Jedis jedis = new Jedis("127.0.0.1", 6379);
Assert.assertNotNull(jedis.info());
System.out.println(jedis.info());
jedis.close();
</code></pre>

<p>}
```</p>

<p>avec le pom suivant :
```xml
<dependency></p>

<pre><code>&lt;groupId&gt;fr.jetoile.hadoop&lt;/groupId&gt;
&lt;artifactId&gt;hadoop-unit-redis&lt;/artifactId&gt;
&lt;version&gt;2.2&lt;/version&gt;
&lt;scope&gt;test&lt;/scope&gt;
</code></pre>

<p></dependency></p>

<p><dependency></p>

<pre><code>&lt;groupId&gt;redis.clients&lt;/groupId&gt;
&lt;artifactId&gt;jedis&lt;/artifactId&gt;
&lt;version&gt;2.9.0&lt;/version&gt;
&lt;scope&gt;test&lt;/scope&gt;
</code></pre>

<p></dependency>
```</p>

<p>et le fichier de configuration <code>hadoop-unit-default.properties</code> suivant:
```plain</p>

<h1>Redis</h1>

<p>redis.port=6379
redis.download.url=<a href="http://download.redis.io/releases/">http://download.redis.io/releases/</a>
redis.version=4.0.0
redis.cleanup.installation=false
redis.temp.dir=/tmp/redis
redis.type=SERVER</p>

<h1>redis.type=CLUSTER</h1>

<h1>redis.type=MASTER_SLAVE</h1>

<h1>redis.type=SENTINEL</h1>

<h1>redis.slave.ports=6380</h1>

<h1>redis.sentinel.ports=36479,36480,36481,36482,36483</h1>

<p>```</p>

<ul>
<li>En utilisant le plugin maven en <a href="https://github.com/jetoile/hadoop-unit/tree/master/sample/redis">phase de pré-integration</a>:
<code>xml
&lt;plugin&gt;
  &lt;artifactId&gt;hadoop-unit-maven-plugin&lt;/artifactId&gt;
  &lt;groupId&gt;fr.jetoile.hadoop&lt;/groupId&gt;
  &lt;version&gt;${hadoop-unit.version}&lt;/version&gt;
  &lt;executions&gt;
      &lt;execution&gt;
          &lt;id&gt;start&lt;/id&gt;
          &lt;goals&gt;
              &lt;goal&gt;embedded-start&lt;/goal&gt;
          &lt;/goals&gt;
          &lt;phase&gt;pre-integration-test&lt;/phase&gt;
      &lt;/execution&gt;
  &lt;/executions&gt;
  &lt;configuration&gt;
      &lt;components&gt;
          &lt;componentArtifact implementation="fr.jetoile.hadoopunit.ComponentArtifact"&gt;
              &lt;componentName&gt;REDIS&lt;/componentName&gt;
              &lt;artifact&gt;fr.jetoile.hadoop:hadoop-unit-redis:${hadoop-unit.version}&lt;/artifact&gt;
          &lt;/componentArtifact&gt;
      &lt;/components&gt;
  &lt;/configuration&gt;
&lt;/plugin&gt;
</code></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Des tests d'intégration avec Elasticsearch]]></title>
    <link href="https://blog.jetoile.fr/2017/07/tester-avec-elasticsearch.html"/>
    <updated>2017-07-11T10:05:51+02:00</updated>
    <id>https://blog.jetoile.fr/2017/07/tester-avec-elasticsearch</id>
    <content type="html"><![CDATA[<p><img src="https://static-www.elastic.co/assets/blteb1c97719574938d/logo-elastic-elasticsearch-lt.svg?q=540" alt="elasticsearch logo" /> La version 5.0.0-alpha4 a signé la fin du support du mode embedded d'Elasticsearch.</p>

<p>Cela a été annoncé <a href="https://www.elastic.co/blog/elasticsearch-the-server#_embedded_elasticsearch_not_supported">là</a> et la classe <code>NodeBuilder</code> permettant de démarrer un noeud programmatiquement a été supprimée.</p>

<p>Cependant, même si la raison de l'arrêt du support de ce mode est compréhensible, cela pose le problème des tests d'intégration puisqu'il n'est plus possible de démarrer un Elasticsearch pendant la phase de test.</p>

<p>Oui, Elastic propose officiellement une <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/testing-framework.html">alternative</a> via l'utilisation de ESIntegTestCase mais personnellement, je ne suis pas très fan de cette approche&hellip;</p>

<p>Cet article va tenter de dresser un panorama non exhaustif de ce que j'ai pu trouver d'intéressant pour permettre de réaliser des tests d'intégration avec Elasticsearch.</p>

<!--more-->


<p>Parmi les solutions intéressantes et simples que j'ai trouvés pour faire des tests d'intégration, il y a surtout 2 projets que j'ai retenus.</p>

<h1>Plugin maven permettant de télécharger, installer et démarrer Elasticsearch</h1>

<p>Le premier se trouve être celui de <a href="https://github.com/alexcojocaru/elasticsearch-maven-plugin">alexcojocaru</a>.</p>

<p>Il s'agit d'un plugin maven s'appuyant sur <a href="https://github.com/apache/maven-resolver">maven-resolver</a> qui permet sur les phases de pré-intégration et post-intégration de démarrer et d'arrêter Elasticsearch.</p>

<p>```xml
<plugin></p>

<pre><code>&lt;groupId&gt;com.github.alexcojocaru&lt;/groupId&gt;
&lt;artifactId&gt;elasticsearch-maven-plugin&lt;/artifactId&gt;
&lt;version&gt;5.7&lt;/version&gt;
&lt;configuration&gt;
    &lt;clusterName&gt;elasticsearch&lt;/clusterName&gt;
    &lt;transportPort&gt;9300&lt;/transportPort&gt;
    &lt;httpPort&gt;9200&lt;/httpPort&gt;
    &lt;autoCreateIndex&gt;true&lt;/autoCreateIndex&gt;
&lt;/configuration&gt;
&lt;executions&gt;
    &lt;execution&gt;
        &lt;id&gt;start-elasticsearch&lt;/id&gt;
        &lt;phase&gt;pre-integration-test&lt;/phase&gt;
        &lt;goals&gt;
            &lt;goal&gt;runforked&lt;/goal&gt;
        &lt;/goals&gt;
    &lt;/execution&gt;
    &lt;execution&gt;
        &lt;id&gt;stop-elasticsearch&lt;/id&gt;
        &lt;phase&gt;post-integration-test&lt;/phase&gt;
        &lt;goals&gt;
            &lt;goal&gt;stop&lt;/goal&gt;
        &lt;/goals&gt;
    &lt;/execution&gt;
&lt;/executions&gt;
</code></pre>

<p></plugin>
```</p>

<h2>Exemple</h2>

<p>Code java de test :
```java
public class ElasticsearchIntegrationTest {</p>

<pre><code>@Test
public void transportClient_should_success() throws IOException, InterruptedException {
    TransportClient client = new PreBuiltTransportClient(Settings.EMPTY)
            .addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName("localhost"), 9300));

    ObjectMapper mapper = new ObjectMapper();

    Sample sample = new Sample("value", 0.33, 3);

    String jsonString = mapper.writeValueAsString(sample);

    // indexing document
    IndexResponse ir = client.prepareIndex("test_index", "type").setSource(jsonString).setId("1").execute().actionGet();
    client.admin().indices().prepareRefresh("test_index").execute().actionGet();

    assertNotNull(ir);

    GetResponse gr = client.prepareGet("test_index", "type", "1").execute().actionGet();

    assertNotNull(gr);
    assertEquals(gr.getSourceAsString(), "{\"value\":\"value\",\"size\":0.33,\"price\":3.0}");
}

@Test
public void restClient_should_success() throws IOException, JSONException {

    RestClient restClient = RestClient.builder(
            new HttpHost("localhost", 9200, "http")).build();

    org.elasticsearch.client.Response response = restClient.performRequest("GET", "/",
            Collections.singletonMap("pretty", "true"));
    System.out.println(EntityUtils.toString(response.getEntity()));

    // indexing document
    HttpEntity entity = new NStringEntity(
            "{\n" +
                    "    \"user\" : \"kimchy\",\n" +
                    "    \"post_date\" : \"2009-11-15T14:12:12\",\n" +
                    "    \"message\" : \"trying out Elasticsearch\"\n" +
                    "}", ContentType.APPLICATION_JSON);

    org.elasticsearch.client.Response indexResponse = restClient.performRequest(
            "PUT",
            "/twitter/tweet/1",
            Collections.&lt;String, String&gt;emptyMap(),
            entity);

    response = restClient.performRequest("GET", "/_search",
            Collections.singletonMap("pretty", "true"));

    String result = EntityUtils.toString(response.getEntity());
    System.out.println(result);
    JSONObject obj = new JSONObject(result);
    int nbResult = obj.getJSONObject("hits").getInt("total");
    assertThat(nbResult).isEqualTo(1);

    restClient.close();
}
</code></pre>

<p>}</p>

<p>@EqualsAndHashCode
@Data
@AllArgsConstructor
class Sample implements Serializable {</p>

<pre><code>private String value;
private double size;
private double price;
</code></pre>

<p>}
```</p>

<p>Dépendence maven (façon gradle) :
```plain
dependencies {</p>

<pre><code>compile group: 'org.apache.logging.log4j', name: 'log4j-to-slf4j', version:'2.7'
compile group: 'org.slf4j', name: 'slf4j-api', version:'1.7.12'
compile group: 'ch.qos.logback', name: 'logback-classic', version:'1.1.3'
testCompile group: 'org.elasticsearch.client', name: 'transport', version:'5.4.3'
testCompile group: 'org.elasticsearch.client', name: 'rest', version:'5.4.3'
testCompile group: 'com.fasterxml.jackson.core', name: 'jackson-core', version:'2.7.1'
testCompile group: 'com.fasterxml.jackson.core', name: 'jackson-databind', version:'2.7.1'
testCompile group: 'org.codehaus.jettison', name: 'jettison', version:'1.3.8'
testCompile group: 'org.assertj', name: 'assertj-core', version:'3.8.0'
testCompile group: 'junit', name: 'junit', version:'4.11'
compile(group: 'org.projectlombok', name: 'lombok', version:'1.16.6') {
   /* This dependency was originally in the Maven provided scope, but the project was not of type war.
   This behavior is not yet supported by Gradle, so this dependency has been converted to a compile dependency.
   Please review and delete this closure when resolved. */
}
</code></pre>

<p>}
```</p>

<p>Plugins maven utilisés :
```xml
<plugin></p>

<pre><code>&lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt;
&lt;configuration&gt;
    &lt;excludes&gt;
        &lt;exclude&gt;**/*IntegrationTest.java&lt;/exclude&gt;
    &lt;/excludes&gt;
&lt;/configuration&gt;
&lt;executions&gt;
    &lt;execution&gt;
        &lt;id&gt;integration-test&lt;/id&gt;
        &lt;goals&gt;
            &lt;goal&gt;test&lt;/goal&gt;
        &lt;/goals&gt;
        &lt;phase&gt;integration-test&lt;/phase&gt;
        &lt;configuration&gt;
            &lt;excludes&gt;
                &lt;exclude&gt;none&lt;/exclude&gt;
            &lt;/excludes&gt;
            &lt;includes&gt;
                &lt;include&gt;**/*IntegrationTest.java&lt;/include&gt;
            &lt;/includes&gt;
        &lt;/configuration&gt;
    &lt;/execution&gt;
&lt;/executions&gt;
</code></pre>

<p></plugin></p>

<p><plugin></p>

<pre><code>&lt;groupId&gt;com.github.alexcojocaru&lt;/groupId&gt;
&lt;artifactId&gt;elasticsearch-maven-plugin&lt;/artifactId&gt;
&lt;version&gt;5.7&lt;/version&gt;
&lt;configuration&gt;
    &lt;clusterName&gt;elasticsearch&lt;/clusterName&gt;
    &lt;transportPort&gt;9300&lt;/transportPort&gt;
    &lt;httpPort&gt;9200&lt;/httpPort&gt;
    &lt;autoCreateIndex&gt;true&lt;/autoCreateIndex&gt;
&lt;/configuration&gt;
&lt;executions&gt;
    &lt;execution&gt;
        &lt;id&gt;start-elasticsearch&lt;/id&gt;
        &lt;phase&gt;pre-integration-test&lt;/phase&gt;
        &lt;goals&gt;
            &lt;goal&gt;runforked&lt;/goal&gt;
        &lt;/goals&gt;
    &lt;/execution&gt;
    &lt;execution&gt;
        &lt;id&gt;stop-elasticsearch&lt;/id&gt;
        &lt;phase&gt;post-integration-test&lt;/phase&gt;
        &lt;goals&gt;
            &lt;goal&gt;stop&lt;/goal&gt;
        &lt;/goals&gt;
    &lt;/execution&gt;
&lt;/executions&gt;
</code></pre>

<p></plugin>
```</p>

<h1>Téléchargement, installation et démarrage d'Elasticsearch</h1>

<p>Le deuxième projet est celui d'<a href="https://github.com/allegro/embedded-elasticsearch">Allegro Tech</a>.</p>

<p>Contrairement à la solution précédente, ce projet permet programmatiquement de télécharger, installer et démarrer/arrêter Elasticsearch.</p>

<p>En outre, l'avantage de cette solution est qu'il n'est pas nécessaire de configurer la partie test d'intégration dans maven. Ainsi, utiliser un autre outils de build est possible.</p>

<p>```java
final embeddedElastic = EmbeddedElastic.builder()</p>

<pre><code>.withElasticVersion("5.4.3")
.withSetting(PopularProperties.TRANSPORT_TCP_PORT, 9300)
.withSetting(PopularProperties.CLUSTER_NAME, "elasticsearch")
.withPlugin("analysis-stempel")
.withIndex("cars", IndexSettings.builder()
    .withType("car", getSystemResourceAsStream("car-mapping.json"))
    .build())
.withIndex("books", IndexSettings.builder()
    .withType(PAPER_BOOK_INDEX_TYPE, getSystemResourceAsStream("paper-book-mapping.json"))
    .withType("audio_book", getSystemResourceAsStream("audio-book-mapping.json"))
    .withSettings(getSystemResourceAsStream("elastic-settings.json"))
    .build())
.build()
.start()
</code></pre>

<p>```</p>

<h1>Exemple</h1>

<p>Code java :
```java
public class ElasticsearchTest {</p>

<pre><code>private static EmbeddedElastic elasticsearchCluster;

@BeforeClass
public static void setup() throws IOException, InterruptedException {
    elasticsearchCluster = EmbeddedElastic.builder()
            .withElasticVersion("5.4.3")
            .withSetting(PopularProperties.TRANSPORT_TCP_PORT, 9300)
            .withSetting(PopularProperties.HTTP_PORT, 9200)
            .withSetting(PopularProperties.CLUSTER_NAME, "elasticsearch")
            .withSetting("network.host", "localhost")
            .withCleanInstallationDirectoryOnStop(true)
            .build()
            .start();
}

@AfterClass
public static void teardown() throws IOException, InterruptedException {
    elasticsearchCluster.stop();
}

@Test
public void transportClient_should_success() throws IOException, InterruptedException {
    TransportClient client = new PreBuiltTransportClient(Settings.EMPTY)
            .addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName("localhost"), 9300));

    ObjectMapper mapper = new ObjectMapper();

    Sample sample = new Sample("value", 0.33, 3);

    String jsonString = mapper.writeValueAsString(sample);

    // indexing document
    IndexResponse ir = client.prepareIndex("test_index", "type").setSource(jsonString).setId("1").execute().actionGet();
    client.admin().indices().prepareRefresh("test_index").execute().actionGet();

    assertNotNull(ir);

    GetResponse gr = client.prepareGet("test_index", "type", "1").execute().actionGet();

    assertNotNull(gr);
    assertEquals(gr.getSourceAsString(), "{\"value\":\"value\",\"size\":0.33,\"price\":3.0}");
}

@Test
public void restClient_should_success() throws IOException, JSONException {

    RestClient restClient = RestClient.builder(
            new HttpHost("localhost", 9200, "http")).build();

    org.elasticsearch.client.Response response = restClient.performRequest("GET", "/",
            Collections.singletonMap("pretty", "true"));
    System.out.println(EntityUtils.toString(response.getEntity()));

    // indexing document
    HttpEntity entity = new NStringEntity(
            "{\n" +
                    "    \"user\" : \"kimchy\",\n" +
                    "    \"post_date\" : \"2009-11-15T14:12:12\",\n" +
                    "    \"message\" : \"trying out Elasticsearch\"\n" +
                    "}", ContentType.APPLICATION_JSON);

    org.elasticsearch.client.Response indexResponse = restClient.performRequest(
            "PUT",
            "/twitter/tweet/1",
            Collections.&lt;String, String&gt;emptyMap(),
            entity);

    response = restClient.performRequest("GET", "/_search",
            Collections.singletonMap("pretty", "true"));

    String result = EntityUtils.toString(response.getEntity());
    System.out.println(result);
    JSONObject obj = new JSONObject(result);
    int nbResult = obj.getJSONObject("hits").getInt("total");
    assertThat(nbResult).isEqualTo(1);

    restClient.close();
}
</code></pre>

<p>}</p>

<p>@EqualsAndHashCode
@Data
@AllArgsConstructor
class Sample implements Serializable {</p>

<pre><code>private String value;
private double size;
private double price;
</code></pre>

<p>}
```</p>

<p>Dépendence maven (façon gradle) :
```plain
dependencies {</p>

<pre><code>compile group: 'org.apache.logging.log4j', name: 'log4j-to-slf4j', version:'2.7'
compile group: 'org.slf4j', name: 'slf4j-api', version:'1.7.12'
compile group: 'ch.qos.logback', name: 'logback-classic', version:'1.1.3'
testCompile group: 'pl.allegro.tech', name: 'embedded-elasticsearch', version:'2.2.0'
testCompile group: 'org.elasticsearch.client', name: 'transport', version:'5.4.3'
testCompile group: 'org.elasticsearch.client', name: 'rest', version:'5.4.3'
testCompile group: 'com.fasterxml.jackson.core', name: 'jackson-core', version:'2.7.1'
testCompile group: 'com.fasterxml.jackson.core', name: 'jackson-databind', version:'2.7.1'
testCompile group: 'org.codehaus.jettison', name: 'jettison', version:'1.3.8'
testCompile group: 'org.assertj', name: 'assertj-core', version:'3.8.0'
testCompile group: 'junit', name: 'junit', version:'4.11'
compile(group: 'org.projectlombok', name: 'lombok', version:'1.16.6') {
   /* This dependency was originally in the Maven provided scope, but the project was not of type war.
   This behavior is not yet supported by Gradle, so this dependency has been converted to a compile dependency.
   Please review and delete this closure when resolved. */
}
</code></pre>

<p>```</p>

<h1>Conclusion</h1>

<p>En conclusion, on a pu voir deux solutions qui permettent de faire des tests d'intégration avec Elasticsearch.</p>

<p>Personnellement, j'ai une préférence pour la deuxième solution qui me permet d'avoir la main sur la récupération et installation d'Elasticsearch.</p>

<p>Pour aller plus loin, quelques liens en vrac.</p>

<ul>
<li><a href="http://david.pilato.fr/blog/2016/10/18/elasticsearch-real-integration-tests-updated-for-ga/">http://david.pilato.fr/blog/2016/10/18/elasticsearch-real-integration-tests-updated-for-ga/</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/testing-framework.html">https://www.elastic.co/guide/en/elasticsearch/reference/current/testing-framework.html</a></li>
<li><a href="https://gquintana.github.io/2016/11/30/Testing-a-Java-and-Elasticsearch-50-application.html">https://gquintana.github.io/2016/11/30/Testing-a-Java-and-Elasticsearch-50-application.html</a></li>
<li><a href="https://github.com/alexcojocaru/elasticsearch-maven-plugin">https://github.com/alexcojocaru/elasticsearch-maven-plugin</a></li>
<li><a href="https://github.com/allegro/embedded-elasticsearch">https://github.com/allegro/embedded-elasticsearch</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
