<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Test | Jetoile]]></title>
  <link href="https://blog.jetoile.fr/blog/categories/test/atom.xml" rel="self"/>
  <link href="https://blog.jetoile.fr/"/>
  <updated>2017-10-19T20:38:49+02:00</updated>
  <id>https://blog.jetoile.fr/</id>
  <author>
    <name><![CDATA[Khanh Maudoux]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Des tests d'intégration avec Cassandra]]></title>
    <link href="https://blog.jetoile.fr/2017/07/tester-avec-cassandra.html"/>
    <updated>2017-07-21T10:22:37+02:00</updated>
    <id>https://blog.jetoile.fr/2017/07/tester-avec-cassandra</id>
    <content type="html"><![CDATA[<p><img src="/images/1200px-Cassandra_logo.svg.png" alt="left-small" /> Parce que je suis parti sur ma lancée des articles <em>des tests d'intégration avec &hellip;</em>, à la demande de <a href="https://twitter.com/doanduyhai">Duyhai</a>, voilà que je me retrouve à faire un article pour Apache Cassandra&hellip; ;)</p>

<p>Plus sérieusement, faire des tests d'intégration avec Apache Cassandra est beaucoup plus simple qu'avec Redis ou Elasticsearch mais il existe cependant 2 projets qui simplifient énormément les tests d'intégration avec Cassandra :</p>

<ul>
<li><a href="https://github.com/jsevellec/cassandra-unit">cassandra-unit</a> créé par <a href="https://github.com/jsevellec">Jérémy Sevellec</a></li>
<li><a href="https://github.com/doanduyhai/Achilles">achille</a> créé par <a href="https://twitter.com/doanduyhai">Duyhai Doan</a></li>
</ul>


<p>Ce petit article résume comment utiliser ces 2 solutions.</p>

<!-- more -->


<h1>Classe de test</h1>

<p>La classe utilisée comme exemple dans les 2 cas est la suivante :</p>

<p>```java
import com.datastax.driver.core.Cluster;
import com.datastax.driver.core.ResultSet;
import com.datastax.driver.core.Row;
import com.datastax.driver.core.Session;
import lombok.AllArgsConstructor;
import lombok.Data;</p>

<p>import java.util.List;
import java.util.stream.Collectors;</p>

<p>public class UserDao {</p>

<pre><code>private Cluster cluster;
private Session session;

public UserDao(String localhost, int port) {
    cluster = Cluster.builder()
            .addContactPoints(localhost)
            .withPort(port)
            .build();

    session = cluster.connect();
}

public List&lt;User&gt; getUsers() {
    ResultSet execute = session.execute("select * from test.user");

    List&lt;Row&gt; res = execute.all();

    return res.stream().map(r -&gt; new User(r.getString("lastName"), r.getString("firstName"))).collect(Collectors.toList());
}
</code></pre>

<p>}</p>

<p>@Data
@AllArgsConstructor
class User {</p>

<pre><code>private String lastName;
private String firstName;
</code></pre>

<p>}
```</p>

<p>Au niveau dépendance, elles sont les suivantes :
```plain
dependencies {</p>

<pre><code>compile group: 'com.datastax.cassandra', name: 'cassandra-driver-core', version:'3.3.0'
compile(group: 'org.projectlombok', name: 'lombok', version:'1.16.18') 
</code></pre>

<p>}
```</p>

<p>Rien de bien compliqué mais il s'agit d'un cas d'exemple ultra simple&hellip; ;)</p>

<h1>Cassandra-Unit</h1>

<p>Pour utiliser Cassandra-Unit, il suffit de déclarer sa dépendance en scope de test:
```xml
<dependency></p>

<pre><code>&lt;groupId&gt;org.cassandraunit&lt;/groupId&gt;
&lt;artifactId&gt;cassandra-unit&lt;/artifactId&gt;
&lt;version&gt;3.1.3.2&lt;/version&gt;
&lt;scope&gt;test&lt;/scope&gt;
</code></pre>

<p></dependency>
```</p>

<p>La classe de test ressemble alors à :
```java
import com.datastax.driver.core.Cluster;
import com.datastax.driver.core.Session;
import org.apache.thrift.transport.TTransportException;
import org.cassandraunit.utils.EmbeddedCassandraServerHelper;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;</p>

<p>import java.io.IOException;
import java.util.List;</p>

<p>import static org.junit.Assert.assertEquals;</p>

<p>public class UserDaoIntegrationTest {</p>

<pre><code>private Session session;
private Cluster cluster;

@Before
public void setUp() throws InterruptedException, IOException, TTransportException {
    EmbeddedCassandraServerHelper.startEmbeddedCassandra("cassandra-sample.yaml");

    cluster = Cluster.builder()
            .addContactPoints("localhost")
            .withPort(9000)
            .build();

    session = cluster.connect();

    session.execute("create KEYSPACE test WITH replication = {'class': 'SimpleStrategy' , 'replication_factor': '1' }");
    session.execute("CREATE TABLE test.user (lastName text, firstName text, PRIMARY KEY (lastName))");
}

@After
public void teardown() {
    session.close();
    EmbeddedCassandraServerHelper.stopEmbeddedCassandra();
}


@Test
public void user_should_be_returned() {
    session.execute("insert into test.user(lastName, firstName) values('lastName1', 'firstName1')");

    UserDao cassandraJob = new UserDao("localhost", 9000);

    List&lt;User&gt; users = cassandraJob.getUsers();

    assertEquals(users.size(), 1);
    assertEquals(users.get(0).getFirstName(), "firstName1");
    assertEquals(users.get(0).getLastName(), "lastName1");
}
</code></pre>

<p>}
```</p>

<p>A noter que le fichier <code>cassandra-sample.yaml</code> correspond au fichier de configuration par défaut de cassandra avec les spécificités suivantes :
```yaml
native_transport_port: 9000
data_file_directories:</p>

<pre><code>- target/embeddedCassandra/data
</code></pre>

<p>commitlog_directory: target/embeddedCassandra/commitlog
hints_directory: target/embeddedCassandra/hints
cdc_raw_directory: target/embeddedCassandra/cdc_raw
saved_caches_directory: target/embeddedCassandra/saved_caches</p>

<h1>multithreaded_compaction: false</h1>

<h1>memtable_flush_queue_size: 4</h1>

<h1>compaction_preheat_key_cache: true</h1>

<h1>in_memory_compaction_limit_in_mb: 64</h1>

<p>```</p>

<p>A noter également que Cassandra-Unit propose également des loader permettant d'initialiser les données dans le cluster ainsi que d'autres facilités d'utilisation telles que des Rules ou un support de Spring. Pour avoir plus d'informations, je vous invite à aller voir la documentation.</p>

<h1>Achille-Embedded</h1>

<p>Pour utiliser Achille-embedded, il suffit de déclarer sa dépendance en scope de test:
```xml
<dependency></p>

<pre><code>&lt;groupId&gt;info.archinnov&lt;/groupId&gt;
&lt;artifactId&gt;achilles-embedded&lt;/artifactId&gt;
&lt;version&gt;5.2.1&lt;/version&gt;
&lt;scope&gt;test&lt;/scope&gt;
</code></pre>

<p></dependency>
```</p>

<p>La classe de test ressemble alors à :
```java
import com.datastax.driver.core.Session;
import info.archinnov.achilles.embedded.CassandraEmbeddedServerBuilder;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;</p>

<p>import java.util.List;</p>

<p>import static org.junit.Assert.assertEquals;</p>

<p>public class UserDaoIntegrationTest {</p>

<pre><code>private Session session;

@Before
public void setUp() {
    session = CassandraEmbeddedServerBuilder.builder()
            .withCQLPort(9000)
            .withClusterName("Test Cluster")
            .cleanDataFilesAtStartup(true)
            .buildNativeSession();

    session.execute("create KEYSPACE test WITH replication = {'class': 'SimpleStrategy' , 'replication_factor': '1' }");
    session.execute("CREATE TABLE test.user (lastName text, firstName text, PRIMARY KEY (lastName))");
}

@After
public void teardown() {
    session.close();
}


@Test
public void user_should_be_returned() {
    session.execute("insert into test.user(lastName, firstName) values('lastName1', 'firstName1')");

    UserDao cassandraJob = new UserDao("localhost", 9000);

    List&lt;User&gt; users = cassandraJob.getUsers();

    assertEquals(users.size(), 1);
    assertEquals(users.get(0).getFirstName(), "firstName1");
    assertEquals(users.get(0).getLastName(), "lastName1");
}
</code></pre>

<p>}
```</p>

<p>A noter également qu'Achille-Embedded propose également des Rules et offre des facilités pour loader des données pour les tests. Pour avoir plus d'informations, je vous invite à aller voir les différents tests du projet.</p>

<h1>Conclusion</h1>

<p>On a donc vu dans ce rapide article comment il était possible de faire des tests d'intégration avec Cassandra.</p>

<p>A noter que le code montré dans cet article est extrêmement simple mais je voulais juste montrer un rapide cas d'usage, du coup, pas grand chose à ajouter puisque les exemples parlent d'eux-même ;) .</p>

<p>Concernant le choix de l'un ou de l'autre, j'avoue avoir une préférence pour Achille-Embedded car il permet plus simplement d'avoir la main sur la configuration et ne pas avoir à maintenir un fichier de configuration en plus.</p>

<p>A noter qu'Achille-Embedded a été utilisé dans <a href="https://github.com/jetoile/hadoop-unit">Hadoop-Unit</a> et qu'il est possible d'utiliser Cassandra en test d'intégration des manière suivantes ( #autopromo ;) ) :</p>

<ul>
<li>En utilisant le démarrage de <a href="https://github.com/jetoile/hadoop-unit/blob/master/hadoop-unit-cassandra/src/test/java/fr/jetoile/hadoopunit/component/CassandraBootstrapTest.java">manière programmatique</a></li>
<li>En utilisant le plugin maven en <a href="https://github.com/jetoile/hadoop-unit/blob/master/sample/spark-streaming-cassandra">phase de pré-integration</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Des tests d'intégration avec Redis]]></title>
    <link href="https://blog.jetoile.fr/2017/07/tester-avec-redis.html"/>
    <updated>2017-07-17T11:10:30+02:00</updated>
    <id>https://blog.jetoile.fr/2017/07/tester-avec-redis</id>
    <content type="html"><![CDATA[<p><img src="/images/redis.png" alt="left-small" /> Redis est écrit en C et faire des tests d'intégration en Java peut s'avérer compliquer. En outre, le fait que Redis doive être compilé lors de son installation rend les choses encore moins aisées.</p>

<p>Bien sûr, il est possible d'utiliser Docker ou de l'installer préalablement sur son poste mais cette deuxième option casse un peu les bonnes pratiques des tests.</p>

<p>Il existe également de nombreux projets permettant de faire des tests avec Redis mais, souvent, les solutions proposées embarquent le binaire de Redis ou on besoin qu'il soit déjà présent et installer/compiler sur le poste (<a href="https://github.com/kstyrc/embedded-redis,">https://github.com/kstyrc/embedded-redis,</a> <a href="https://github.com/lordofthejars/nosql-unit,">https://github.com/lordofthejars/nosql-unit,</a> <a href="https://github.com/ishiis/redis-unit">https://github.com/ishiis/redis-unit</a>). Les solutions qui intègrent le binaire ne sont malheureusement souvent pas à jour et laisse assez peu la main sur la version.</p>

<p>Pour ceux qui n'aurait pas envie d'utiliser Docker, cet article va montrer comment il est possible de piloter programmatiquement l'installation de Redis afin de permettre les tests d'intégration.</p>

<!-- more -->


<p>En fait, le code ci-dessous se charge de télécharger Redis et de le compiler dans le répertoire <code>$USER_HOME/.redis</code>. Pour ce faire une tache ant est utilisée car un appel direct à la commande make échoue.</p>

<p>```java
import org.apache.commons.io.FileUtils;
import org.apache.commons.io.IOUtils;
import org.apache.tools.ant.BuildException;
import org.apache.tools.ant.DefaultLogger;
import org.apache.tools.ant.Project;
import org.apache.tools.ant.ProjectHelper;
import org.codehaus.plexus.archiver.ArchiverException;
import org.codehaus.plexus.archiver.tar.TarGZipUnArchiver;
import org.codehaus.plexus.logging.console.ConsoleLogger;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;</p>

<p>import java.io.File;
import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;
import java.net.URL;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.util.Arrays;
import java.util.List;</p>

<p>import static org.apache.commons.io.FileUtils.getFile;</p>

<p>public class RedisInstaller {</p>

<pre><code>private static final Logger LOGGER = LoggerFactory.getLogger(RedisInstaller.class);
private static final String REDIS_PACKAGE_PREFIX = "redis-";
private static final List&lt;String&gt; REDIS_EXECUTABLE_FILE = Arrays.asList("redis-server", "redis-sentinel");

private static final Path REDIS_INSTALLATION_PATH = Paths.get(System.getProperty("user.home") + "/.redis");

private final String downloadUrl;
private final String version;
private final boolean forceCleanupInstallationDirectory;
private final String tmpDir;

RedisInstaller(String version, String downloadUrl, boolean forceCleanupInstallationDirectory, String tmpDir) {
    this.downloadUrl = downloadUrl;
    this.version = version;
    this.forceCleanupInstallationDirectory = forceCleanupInstallationDirectory;
    this.tmpDir = tmpDir;
    REDIS_INSTALLATION_PATH.toFile().mkdirs();
}

File getExecutableFile() {
    return fileRelativeToInstallationDir("src", "redis-server");
}

private File fileRelativeToInstallationDir(String... path) {
    return getFile(getInstallationDirectory(), path);
}

private File getInstallationDirectory() {
    return getFile(REDIS_INSTALLATION_PATH.toFile(), REDIS_PACKAGE_PREFIX + version);
}

void install() throws IOException, InterruptedException {
    if (forceCleanupInstallationDirectory) {
        FileUtils.forceDelete(getInstallationDirectory());
    }
    installRedis();
    makeRedis();
    applyRedisPermissionRights();
}


private void installRedis() throws IOException {
    Path downloadedTo = download(new URL(downloadUrl + REDIS_PACKAGE_PREFIX + version + ".tar.gz"));
    install(downloadedTo);
}

private File getAntFile() throws IOException {
    InputStream in = RedisInstaller.class.getClassLoader().getResourceAsStream("build.xml");
    File fileOut = new File(tmpDir, "build.xml");

    LOGGER.info("Writing redis' build.xml to: " + fileOut.getAbsolutePath());

    OutputStream out = FileUtils.openOutputStream(fileOut);
    IOUtils.copy(in, out);
    in.close();
    out.close();

    return fileOut;
}

private void makeRedis() throws IOException, InterruptedException {
    LOGGER.info("&gt; make");
    File makeFilePath = getInstallationDirectory();

    DefaultLogger consoleLogger = getConsoleLogger();

    Project project = new Project();
    File buildFile = getAntFile();
    project.setUserProperty("ant.file", buildFile.getAbsolutePath());
    project.addBuildListener(consoleLogger);

    try {
        project.fireBuildStarted();
        project.init();
        ProjectHelper projectHelper = ProjectHelper.getProjectHelper();
        project.addReference("ant.projectHelper", projectHelper);
        project.setProperty("redisDirectory", makeFilePath.getAbsolutePath());
        projectHelper.parse(project, buildFile);
        project.executeTarget("init");
        project.fireBuildFinished(null);
    } catch (BuildException buildException) {
        project.fireBuildFinished(buildException);
        throw new RuntimeException("!!! Unable to compile redis !!!", buildException);
    }
}

private DefaultLogger getConsoleLogger() {
    DefaultLogger consoleLogger = new DefaultLogger();
    consoleLogger.setErrorPrintStream(System.err);
    consoleLogger.setOutputPrintStream(System.out);
    consoleLogger.setMessageOutputLevel(Project.MSG_INFO);

    return consoleLogger;
}

private Path download(URL source) throws IOException {
    File target = new File(REDIS_INSTALLATION_PATH.toString(), source.getPath());
    if (!target.exists()) {
        LOGGER.info("Downloading : " + source + " to " + target + "...");
        FileUtils.copyURLToFile(source, target);
        LOGGER.info("Download complete");
    } else {
        LOGGER.info("Download skipped");
    }
    return target.toPath();
}

private void install(Path downloadedFile) throws IOException {
    LOGGER.info("Installing Redis into " + REDIS_INSTALLATION_PATH + "...");
    try {
        final TarGZipUnArchiver ua = new TarGZipUnArchiver();
        ua.setSourceFile(downloadedFile.toFile());
        ua.enableLogging(new ConsoleLogger(1, "console"));
        ua.setDestDirectory(REDIS_INSTALLATION_PATH.toFile());
        ua.extract();
        LOGGER.info("Done");
    } catch (ArchiverException e) {
        LOGGER.info("Failure : " + e);
        throw new RuntimeException("!!! Unable to download and untar redis !!!", e);
    }
}

private void applyRedisPermissionRights() throws IOException {
    File binDirectory = getFile(getInstallationDirectory(), "src");
    for (String fn : REDIS_EXECUTABLE_FILE) {
        File executableFile = new File(binDirectory, fn);
        LOGGER.info("Applying executable permissions on " + executableFile);
        executableFile.setExecutable(true);
    }
}
</code></pre>

<p>}
```</p>

<p>Avec le fichier ant <code>build.xml</code> suivant :
```xml
&lt;?xml version=&ldquo;1.0&rdquo; encoding=&ldquo;ISO-8859-1&rdquo;?>
<project name="make" default="init" basedir="."></p>

<pre><code>&lt;target name="init"&gt;
    &lt;echo message="Redis compilation is starting" /&gt;
    &lt;exec command="make" dir="${redisDirectory}" /&gt;
    &lt;echo message="Redis compilation finished"/&gt;
&lt;/target&gt;
</code></pre>

<p></project>
```</p>

<p>Ainsi, en wrappant l'appel dans un builder, il devient facile d'invoquer l'installeur.
```java
import lombok.Builder;
import lombok.Data;</p>

<p>import java.io.File;
import java.io.IOException;</p>

<p>@Data
@Builder
public class EmbeddedRedisInstaller {</p>

<pre><code>private String downloadUrl;
private String version;
private String tmpDir;
private boolean forceCleanupInstallationDirectory;

RedisInstaller redisInstaller;

public EmbeddedRedisInstaller install() throws IOException, InterruptedException {
    installRedis();
    return this;
}

private void installRedis() throws IOException, InterruptedException {
    redisInstaller = new RedisInstaller(version, downloadUrl, forceCleanupInstallationDirectory, tmpDir);
    redisInstaller.install();
}

public File getExecutableFile() {
    return redisInstaller.getExecutableFile();
}
</code></pre>

<p>}
```</p>

<p>Utilisation de l'installeur :
```java
EmbeddedRedisInstaller.builder()</p>

<pre><code>            .version("4.0.0")
            .downloadUrl("http://download.redis.io/releases/")
            .forceCleanupInstallationDirectory(false)
            .tmpDir("tmp/redis")
            .build()
    .install();
</code></pre>

<p>```</p>

<p>Enfin, en le couplant, avec le projet <a href="https://github.com/ishiis/redis-unit">redis-unit</a>, il est possible de choisir son mode d'utilisation tout en rendant configurable les ports :
```
EmbeddedRedisInstaller installer = EmbeddedRedisInstaller.builder()</p>

<pre><code>            .downloadUrl(downloadUrl)
            .forceCleanupInstallationDirectory(cleanupInstallation)
            .version(version)
            .tmpDir(tmpDir)
            .build();
</code></pre>

<p>try {</p>

<pre><code>installer.install();
</code></pre>

<p>} catch (IOException | InterruptedException e) {</p>

<pre><code>LOGGER.error("unable to install redis", e);
</code></pre>

<p>}</p>

<p>switch (type) {</p>

<pre><code>case SERVER:
    redisServer = new RedisServer(
            new RedisServerConfig.ServerBuilder(masterPort)
                    .redisBinaryPath(installer.getExecutableFile().getAbsolutePath())
                    .build()
    );
    break;
case CLUSTER:
    redisCluster = new RedisCluster(
            slavePorts.stream().map(p -&gt;
                    new RedisClusterConfig.ClusterBuilder(p)
                            .redisBinaryPath(installer.getExecutableFile().getAbsolutePath())
                            .build()
            ).collect(Collectors.toList())
    );
    break;
case MASTER_SLAVE:
    redisMasterSlave = new RedisMasterSlave(
            new RedisMasterSlaveConfig.MasterBuilder(masterPort)
                    .redisBinaryPath(installer.getExecutableFile().getAbsolutePath())
                    .build(),
            slavePorts.stream().map(p -&gt;
                    new RedisMasterSlaveConfig.SlaveBuilder(p, masterPort)
                            .redisBinaryPath(installer.getExecutableFile().getAbsolutePath())
                            .build()
            ).collect(Collectors.toList())
    );
    break;
case SENTINEL:
    redisSentinel = new RedisSentinel(
            new RedisMasterSlaveConfig.MasterBuilder(masterPort)
                    .redisBinaryPath(installer.getExecutableFile().getAbsolutePath())
                    .build(),
            slavePorts.stream().map(p -&gt;
                    new RedisMasterSlaveConfig.SlaveBuilder(p, masterPort)
                            .redisBinaryPath(installer.getExecutableFile().getAbsolutePath())
                            .build()
            ).collect(Collectors.toList()),
            sentinelPorts.stream().map(p -&gt;
                    new RedisSentinelConfig.SentinelBuilder(p, masterPort)
                            .redisBinaryPath(installer.getExecutableFile().getAbsolutePath())
                            .build()
            ).collect(Collectors.toList())
    );
    break;
</code></pre>

<p>}
```</p>

<h1>Conclusion</h1>

<p>On a pu voir dans cet article comment il était possible simplement d'utiliser Redis lors de ses tests d'intégration sans utiliser Docker et tout en pilotant la chose programmatiquement.</p>

<p>Pour retrouver le code, il est disponible à l'<a href="https://github.com/jetoile/hadoop-unit/tree/master/hadoop-unit-redis">url suivante</a>.</p>

<p>A noter qu'il a été utilisé dans <a href="https://github.com/jetoile/hadoop-unit">Hadoop-Unit</a> et qu'il est possible d'utiliser Redis en test d'intégration des manière suivantes ( #autopromo ;) ) :</p>

<ul>
<li>En utilisant le démarrage de <a href="https://github.com/jetoile/hadoop-unit/blob/master/hadoop-unit-redis/src/test/java/fr/jetoile/hadoopunit/RedisBootstrapTest.java">manière programmatique</a>:
```
@BeforeClass
public static void setup() throws BootstrapException {
  HadoopBootstrap.INSTANCE.startAll();
}</li>
</ul>


<p>@AfterClass
public static void tearDown() throws BootstrapException {</p>

<pre><code>HadoopBootstrap.INSTANCE.stopAll();
</code></pre>

<p>}</p>

<p>@Test
public void testStartAndStopServerMode() throws InterruptedException {</p>

<pre><code>Jedis jedis = new Jedis("127.0.0.1", 6379);
Assert.assertNotNull(jedis.info());
System.out.println(jedis.info());
jedis.close();
</code></pre>

<p>}
```</p>

<p>avec le pom suivant :
```xml
<dependency></p>

<pre><code>&lt;groupId&gt;fr.jetoile.hadoop&lt;/groupId&gt;
&lt;artifactId&gt;hadoop-unit-redis&lt;/artifactId&gt;
&lt;version&gt;2.2&lt;/version&gt;
&lt;scope&gt;test&lt;/scope&gt;
</code></pre>

<p></dependency></p>

<p><dependency></p>

<pre><code>&lt;groupId&gt;redis.clients&lt;/groupId&gt;
&lt;artifactId&gt;jedis&lt;/artifactId&gt;
&lt;version&gt;2.9.0&lt;/version&gt;
&lt;scope&gt;test&lt;/scope&gt;
</code></pre>

<p></dependency>
```</p>

<p>et le fichier de configuration <code>hadoop-unit-default.properties</code> suivant:
```plain</p>

<h1>Redis</h1>

<p>redis.port=6379
redis.download.url=<a href="http://download.redis.io/releases/">http://download.redis.io/releases/</a>
redis.version=4.0.0
redis.cleanup.installation=false
redis.temp.dir=/tmp/redis
redis.type=SERVER</p>

<h1>redis.type=CLUSTER</h1>

<h1>redis.type=MASTER_SLAVE</h1>

<h1>redis.type=SENTINEL</h1>

<h1>redis.slave.ports=6380</h1>

<h1>redis.sentinel.ports=36479,36480,36481,36482,36483</h1>

<p>```</p>

<ul>
<li>En utilisant le plugin maven en <a href="https://github.com/jetoile/hadoop-unit/tree/master/sample/redis">phase de pré-integration</a>:
<code>xml
&lt;plugin&gt;
  &lt;artifactId&gt;hadoop-unit-maven-plugin&lt;/artifactId&gt;
  &lt;groupId&gt;fr.jetoile.hadoop&lt;/groupId&gt;
  &lt;version&gt;${hadoop-unit.version}&lt;/version&gt;
  &lt;executions&gt;
      &lt;execution&gt;
          &lt;id&gt;start&lt;/id&gt;
          &lt;goals&gt;
              &lt;goal&gt;embedded-start&lt;/goal&gt;
          &lt;/goals&gt;
          &lt;phase&gt;pre-integration-test&lt;/phase&gt;
      &lt;/execution&gt;
  &lt;/executions&gt;
  &lt;configuration&gt;
      &lt;components&gt;
          &lt;componentArtifact implementation="fr.jetoile.hadoopunit.ComponentArtifact"&gt;
              &lt;componentName&gt;REDIS&lt;/componentName&gt;
              &lt;artifact&gt;fr.jetoile.hadoop:hadoop-unit-redis:${hadoop-unit.version}&lt;/artifact&gt;
          &lt;/componentArtifact&gt;
      &lt;/components&gt;
  &lt;/configuration&gt;
&lt;/plugin&gt;
</code></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Des tests d'intégration avec Elasticsearch]]></title>
    <link href="https://blog.jetoile.fr/2017/07/tester-avec-elasticsearch.html"/>
    <updated>2017-07-11T10:05:51+02:00</updated>
    <id>https://blog.jetoile.fr/2017/07/tester-avec-elasticsearch</id>
    <content type="html"><![CDATA[<p><img src="https://static-www.elastic.co/assets/blteb1c97719574938d/logo-elastic-elasticsearch-lt.svg?q=540" alt="elasticsearch logo" /> La version 5.0.0-alpha4 a signé la fin du support du mode embedded d'Elasticsearch.</p>

<p>Cela a été annoncé <a href="https://www.elastic.co/blog/elasticsearch-the-server#_embedded_elasticsearch_not_supported">là</a> et la classe <code>NodeBuilder</code> permettant de démarrer un noeud programmatiquement a été supprimée.</p>

<p>Cependant, même si la raison de l'arrêt du support de ce mode est compréhensible, cela pose le problème des tests d'intégration puisqu'il n'est plus possible de démarrer un Elasticsearch pendant la phase de test.</p>

<p>Oui, Elastic propose officiellement une <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/testing-framework.html">alternative</a> via l'utilisation de ESIntegTestCase mais personnellement, je ne suis pas très fan de cette approche&hellip;</p>

<p>Cet article va tenter de dresser un panorama non exhaustif de ce que j'ai pu trouver d'intéressant pour permettre de réaliser des tests d'intégration avec Elasticsearch.</p>

<!--more-->


<p>Parmi les solutions intéressantes et simples que j'ai trouvés pour faire des tests d'intégration, il y a surtout 2 projets que j'ai retenus.</p>

<h1>Plugin maven permettant de télécharger, installer et démarrer Elasticsearch</h1>

<p>Le premier se trouve être celui de <a href="https://github.com/alexcojocaru/elasticsearch-maven-plugin">alexcojocaru</a>.</p>

<p>Il s'agit d'un plugin maven s'appuyant sur <a href="https://github.com/apache/maven-resolver">maven-resolver</a> qui permet sur les phases de pré-intégration et post-intégration de démarrer et d'arrêter Elasticsearch.</p>

<p>```xml
<plugin></p>

<pre><code>&lt;groupId&gt;com.github.alexcojocaru&lt;/groupId&gt;
&lt;artifactId&gt;elasticsearch-maven-plugin&lt;/artifactId&gt;
&lt;version&gt;5.7&lt;/version&gt;
&lt;configuration&gt;
    &lt;clusterName&gt;elasticsearch&lt;/clusterName&gt;
    &lt;transportPort&gt;9300&lt;/transportPort&gt;
    &lt;httpPort&gt;9200&lt;/httpPort&gt;
    &lt;autoCreateIndex&gt;true&lt;/autoCreateIndex&gt;
&lt;/configuration&gt;
&lt;executions&gt;
    &lt;execution&gt;
        &lt;id&gt;start-elasticsearch&lt;/id&gt;
        &lt;phase&gt;pre-integration-test&lt;/phase&gt;
        &lt;goals&gt;
            &lt;goal&gt;runforked&lt;/goal&gt;
        &lt;/goals&gt;
    &lt;/execution&gt;
    &lt;execution&gt;
        &lt;id&gt;stop-elasticsearch&lt;/id&gt;
        &lt;phase&gt;post-integration-test&lt;/phase&gt;
        &lt;goals&gt;
            &lt;goal&gt;stop&lt;/goal&gt;
        &lt;/goals&gt;
    &lt;/execution&gt;
&lt;/executions&gt;
</code></pre>

<p></plugin>
```</p>

<h2>Exemple</h2>

<p>Code java de test :
```java
public class ElasticsearchIntegrationTest {</p>

<pre><code>@Test
public void transportClient_should_success() throws IOException, InterruptedException {
    TransportClient client = new PreBuiltTransportClient(Settings.EMPTY)
            .addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName("localhost"), 9300));

    ObjectMapper mapper = new ObjectMapper();

    Sample sample = new Sample("value", 0.33, 3);

    String jsonString = mapper.writeValueAsString(sample);

    // indexing document
    IndexResponse ir = client.prepareIndex("test_index", "type").setSource(jsonString).setId("1").execute().actionGet();
    client.admin().indices().prepareRefresh("test_index").execute().actionGet();

    assertNotNull(ir);

    GetResponse gr = client.prepareGet("test_index", "type", "1").execute().actionGet();

    assertNotNull(gr);
    assertEquals(gr.getSourceAsString(), "{\"value\":\"value\",\"size\":0.33,\"price\":3.0}");
}

@Test
public void restClient_should_success() throws IOException, JSONException {

    RestClient restClient = RestClient.builder(
            new HttpHost("localhost", 9200, "http")).build();

    org.elasticsearch.client.Response response = restClient.performRequest("GET", "/",
            Collections.singletonMap("pretty", "true"));
    System.out.println(EntityUtils.toString(response.getEntity()));

    // indexing document
    HttpEntity entity = new NStringEntity(
            "{\n" +
                    "    \"user\" : \"kimchy\",\n" +
                    "    \"post_date\" : \"2009-11-15T14:12:12\",\n" +
                    "    \"message\" : \"trying out Elasticsearch\"\n" +
                    "}", ContentType.APPLICATION_JSON);

    org.elasticsearch.client.Response indexResponse = restClient.performRequest(
            "PUT",
            "/twitter/tweet/1",
            Collections.&lt;String, String&gt;emptyMap(),
            entity);

    response = restClient.performRequest("GET", "/_search",
            Collections.singletonMap("pretty", "true"));

    String result = EntityUtils.toString(response.getEntity());
    System.out.println(result);
    JSONObject obj = new JSONObject(result);
    int nbResult = obj.getJSONObject("hits").getInt("total");
    assertThat(nbResult).isEqualTo(1);

    restClient.close();
}
</code></pre>

<p>}</p>

<p>@EqualsAndHashCode
@Data
@AllArgsConstructor
class Sample implements Serializable {</p>

<pre><code>private String value;
private double size;
private double price;
</code></pre>

<p>}
```</p>

<p>Dépendence maven (façon gradle) :
```plain
dependencies {</p>

<pre><code>compile group: 'org.apache.logging.log4j', name: 'log4j-to-slf4j', version:'2.7'
compile group: 'org.slf4j', name: 'slf4j-api', version:'1.7.12'
compile group: 'ch.qos.logback', name: 'logback-classic', version:'1.1.3'
testCompile group: 'org.elasticsearch.client', name: 'transport', version:'5.4.3'
testCompile group: 'org.elasticsearch.client', name: 'rest', version:'5.4.3'
testCompile group: 'com.fasterxml.jackson.core', name: 'jackson-core', version:'2.7.1'
testCompile group: 'com.fasterxml.jackson.core', name: 'jackson-databind', version:'2.7.1'
testCompile group: 'org.codehaus.jettison', name: 'jettison', version:'1.3.8'
testCompile group: 'org.assertj', name: 'assertj-core', version:'3.8.0'
testCompile group: 'junit', name: 'junit', version:'4.11'
compile(group: 'org.projectlombok', name: 'lombok', version:'1.16.6') {
   /* This dependency was originally in the Maven provided scope, but the project was not of type war.
   This behavior is not yet supported by Gradle, so this dependency has been converted to a compile dependency.
   Please review and delete this closure when resolved. */
}
</code></pre>

<p>}
```</p>

<p>Plugins maven utilisés :
```xml
<plugin></p>

<pre><code>&lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt;
&lt;configuration&gt;
    &lt;excludes&gt;
        &lt;exclude&gt;**/*IntegrationTest.java&lt;/exclude&gt;
    &lt;/excludes&gt;
&lt;/configuration&gt;
&lt;executions&gt;
    &lt;execution&gt;
        &lt;id&gt;integration-test&lt;/id&gt;
        &lt;goals&gt;
            &lt;goal&gt;test&lt;/goal&gt;
        &lt;/goals&gt;
        &lt;phase&gt;integration-test&lt;/phase&gt;
        &lt;configuration&gt;
            &lt;excludes&gt;
                &lt;exclude&gt;none&lt;/exclude&gt;
            &lt;/excludes&gt;
            &lt;includes&gt;
                &lt;include&gt;**/*IntegrationTest.java&lt;/include&gt;
            &lt;/includes&gt;
        &lt;/configuration&gt;
    &lt;/execution&gt;
&lt;/executions&gt;
</code></pre>

<p></plugin></p>

<p><plugin></p>

<pre><code>&lt;groupId&gt;com.github.alexcojocaru&lt;/groupId&gt;
&lt;artifactId&gt;elasticsearch-maven-plugin&lt;/artifactId&gt;
&lt;version&gt;5.7&lt;/version&gt;
&lt;configuration&gt;
    &lt;clusterName&gt;elasticsearch&lt;/clusterName&gt;
    &lt;transportPort&gt;9300&lt;/transportPort&gt;
    &lt;httpPort&gt;9200&lt;/httpPort&gt;
    &lt;autoCreateIndex&gt;true&lt;/autoCreateIndex&gt;
&lt;/configuration&gt;
&lt;executions&gt;
    &lt;execution&gt;
        &lt;id&gt;start-elasticsearch&lt;/id&gt;
        &lt;phase&gt;pre-integration-test&lt;/phase&gt;
        &lt;goals&gt;
            &lt;goal&gt;runforked&lt;/goal&gt;
        &lt;/goals&gt;
    &lt;/execution&gt;
    &lt;execution&gt;
        &lt;id&gt;stop-elasticsearch&lt;/id&gt;
        &lt;phase&gt;post-integration-test&lt;/phase&gt;
        &lt;goals&gt;
            &lt;goal&gt;stop&lt;/goal&gt;
        &lt;/goals&gt;
    &lt;/execution&gt;
&lt;/executions&gt;
</code></pre>

<p></plugin>
```</p>

<h1>Téléchargement, installation et démarrage d'Elasticsearch</h1>

<p>Le deuxième projet est celui d'<a href="https://github.com/allegro/embedded-elasticsearch">Allegro Tech</a>.</p>

<p>Contrairement à la solution précédente, ce projet permet programmatiquement de télécharger, installer et démarrer/arrêter Elasticsearch.</p>

<p>En outre, l'avantage de cette solution est qu'il n'est pas nécessaire de configurer la partie test d'intégration dans maven. Ainsi, utiliser un autre outils de build est possible.</p>

<p>```java
final embeddedElastic = EmbeddedElastic.builder()</p>

<pre><code>.withElasticVersion("5.4.3")
.withSetting(PopularProperties.TRANSPORT_TCP_PORT, 9300)
.withSetting(PopularProperties.CLUSTER_NAME, "elasticsearch")
.withPlugin("analysis-stempel")
.withIndex("cars", IndexSettings.builder()
    .withType("car", getSystemResourceAsStream("car-mapping.json"))
    .build())
.withIndex("books", IndexSettings.builder()
    .withType(PAPER_BOOK_INDEX_TYPE, getSystemResourceAsStream("paper-book-mapping.json"))
    .withType("audio_book", getSystemResourceAsStream("audio-book-mapping.json"))
    .withSettings(getSystemResourceAsStream("elastic-settings.json"))
    .build())
.build()
.start()
</code></pre>

<p>```</p>

<h1>Exemple</h1>

<p>Code java :
```java
public class ElasticsearchTest {</p>

<pre><code>private static EmbeddedElastic elasticsearchCluster;

@BeforeClass
public static void setup() throws IOException, InterruptedException {
    elasticsearchCluster = EmbeddedElastic.builder()
            .withElasticVersion("5.4.3")
            .withSetting(PopularProperties.TRANSPORT_TCP_PORT, 9300)
            .withSetting(PopularProperties.HTTP_PORT, 9200)
            .withSetting(PopularProperties.CLUSTER_NAME, "elasticsearch")
            .withSetting("network.host", "localhost")
            .withCleanInstallationDirectoryOnStop(true)
            .build()
            .start();
}

@AfterClass
public static void teardown() throws IOException, InterruptedException {
    elasticsearchCluster.stop();
}

@Test
public void transportClient_should_success() throws IOException, InterruptedException {
    TransportClient client = new PreBuiltTransportClient(Settings.EMPTY)
            .addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName("localhost"), 9300));

    ObjectMapper mapper = new ObjectMapper();

    Sample sample = new Sample("value", 0.33, 3);

    String jsonString = mapper.writeValueAsString(sample);

    // indexing document
    IndexResponse ir = client.prepareIndex("test_index", "type").setSource(jsonString).setId("1").execute().actionGet();
    client.admin().indices().prepareRefresh("test_index").execute().actionGet();

    assertNotNull(ir);

    GetResponse gr = client.prepareGet("test_index", "type", "1").execute().actionGet();

    assertNotNull(gr);
    assertEquals(gr.getSourceAsString(), "{\"value\":\"value\",\"size\":0.33,\"price\":3.0}");
}

@Test
public void restClient_should_success() throws IOException, JSONException {

    RestClient restClient = RestClient.builder(
            new HttpHost("localhost", 9200, "http")).build();

    org.elasticsearch.client.Response response = restClient.performRequest("GET", "/",
            Collections.singletonMap("pretty", "true"));
    System.out.println(EntityUtils.toString(response.getEntity()));

    // indexing document
    HttpEntity entity = new NStringEntity(
            "{\n" +
                    "    \"user\" : \"kimchy\",\n" +
                    "    \"post_date\" : \"2009-11-15T14:12:12\",\n" +
                    "    \"message\" : \"trying out Elasticsearch\"\n" +
                    "}", ContentType.APPLICATION_JSON);

    org.elasticsearch.client.Response indexResponse = restClient.performRequest(
            "PUT",
            "/twitter/tweet/1",
            Collections.&lt;String, String&gt;emptyMap(),
            entity);

    response = restClient.performRequest("GET", "/_search",
            Collections.singletonMap("pretty", "true"));

    String result = EntityUtils.toString(response.getEntity());
    System.out.println(result);
    JSONObject obj = new JSONObject(result);
    int nbResult = obj.getJSONObject("hits").getInt("total");
    assertThat(nbResult).isEqualTo(1);

    restClient.close();
}
</code></pre>

<p>}</p>

<p>@EqualsAndHashCode
@Data
@AllArgsConstructor
class Sample implements Serializable {</p>

<pre><code>private String value;
private double size;
private double price;
</code></pre>

<p>}
```</p>

<p>Dépendence maven (façon gradle) :
```plain
dependencies {</p>

<pre><code>compile group: 'org.apache.logging.log4j', name: 'log4j-to-slf4j', version:'2.7'
compile group: 'org.slf4j', name: 'slf4j-api', version:'1.7.12'
compile group: 'ch.qos.logback', name: 'logback-classic', version:'1.1.3'
testCompile group: 'pl.allegro.tech', name: 'embedded-elasticsearch', version:'2.2.0'
testCompile group: 'org.elasticsearch.client', name: 'transport', version:'5.4.3'
testCompile group: 'org.elasticsearch.client', name: 'rest', version:'5.4.3'
testCompile group: 'com.fasterxml.jackson.core', name: 'jackson-core', version:'2.7.1'
testCompile group: 'com.fasterxml.jackson.core', name: 'jackson-databind', version:'2.7.1'
testCompile group: 'org.codehaus.jettison', name: 'jettison', version:'1.3.8'
testCompile group: 'org.assertj', name: 'assertj-core', version:'3.8.0'
testCompile group: 'junit', name: 'junit', version:'4.11'
compile(group: 'org.projectlombok', name: 'lombok', version:'1.16.6') {
   /* This dependency was originally in the Maven provided scope, but the project was not of type war.
   This behavior is not yet supported by Gradle, so this dependency has been converted to a compile dependency.
   Please review and delete this closure when resolved. */
}
</code></pre>

<p>```</p>

<h1>Conclusion</h1>

<p>En conclusion, on a pu voir deux solutions qui permettent de faire des tests d'intégration avec Elasticsearch.</p>

<p>Personnellement, j'ai une préférence pour la deuxième solution qui me permet d'avoir la main sur la récupération et installation d'Elasticsearch.</p>

<p>Pour aller plus loin, quelques liens en vrac.</p>

<ul>
<li><a href="http://david.pilato.fr/blog/2016/10/18/elasticsearch-real-integration-tests-updated-for-ga/">http://david.pilato.fr/blog/2016/10/18/elasticsearch-real-integration-tests-updated-for-ga/</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/testing-framework.html">https://www.elastic.co/guide/en/elasticsearch/reference/current/testing-framework.html</a></li>
<li><a href="https://gquintana.github.io/2016/11/30/Testing-a-Java-and-Elasticsearch-50-application.html">https://gquintana.github.io/2016/11/30/Testing-a-Java-and-Elasticsearch-50-application.html</a></li>
<li><a href="https://github.com/alexcojocaru/elasticsearch-maven-plugin">https://github.com/alexcojocaru/elasticsearch-maven-plugin</a></li>
<li><a href="https://github.com/allegro/embedded-elasticsearch">https://github.com/allegro/embedded-elasticsearch</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
